{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda7f30a",
   "metadata": {},
   "source": [
    "# Graph Convolutional Neural Network\n",
    "\n",
    "We will use a simple GCN layer and replicate the experiments on the Cora citation dataset. For a high-level explanation on GCN, have a look at its blog pos: https://tkipf.github.io/graph-convolutional-networks/\n",
    "\n",
    "We first need to load the Cora dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c9200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe7ffd",
   "metadata": {},
   "source": [
    "Note that we do not need to use transforms or a dataloader. Now let’s implement a two-layer GCN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "#from torch_geometric.nn import Explainer, GCNConv, to_captum_model\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf990e3",
   "metadata": {},
   "source": [
    "The constructor defines two GCNConv layers which get called in the forward pass of our network. Note that the non-linearity is not integrated in the conv calls and hence needs to be applied afterwards (something which is consistent accross all operators in PyG). Here, we chose to use ReLU as our intermediate non-linearity and finally output a softmax distribution over the number of classes. Let’s train this model on the training nodes for 200 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc4b38",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the test nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connections of the node 1 with the other 2707 nodes:\n",
    "data.edge_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99964305",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.edge_index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.edge_index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0b32e",
   "metadata": {},
   "source": [
    "# Edge Explainability (to be tested after having introduced the graph)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f1ab76e",
   "metadata": {},
   "source": [
    "# Captum assumes that for all given input tensors, dimension 0 is\n",
    "# equal to the number of samples. Therefore, we use unsqueeze(0).\n",
    "captum_model = to_captum_model(model, mask_type='edge', output_idx=output_idx)\n",
    "edge_mask = torch.ones(data.num_edges, requires_grad=True, device=device)\n",
    "\n",
    "ig = IntegratedGradients(captum_model)\n",
    "ig_attr_edge = ig.attribute(edge_mask.unsqueeze(0), target=target,\n",
    "                            additional_forward_args=(data.x, data.edge_index),\n",
    "                            internal_batch_size=1)\n",
    "\n",
    "# Scale attributions to [0, 1]:\n",
    "ig_attr_edge = ig_attr_edge.squeeze(0).abs()\n",
    "ig_attr_edge /= ig_attr_edge.max()\n",
    "\n",
    "# Visualize absolute values of attributions:\n",
    "explainer = Explainer(model)\n",
    "ax, G = explainer.visualize_subgraph(output_idx, data.edge_index, ig_attr_edge)\n",
    "plt.show()\n",
    "\n",
    "# Node explainability\n",
    "# ===================\n",
    "\n",
    "captum_model = to_captum_model(model, mask_type='node', output_idx=output_idx)\n",
    "\n",
    "ig = IntegratedGradients(captum_model)\n",
    "ig_attr_node = ig.attribute(data.x.unsqueeze(0), target=target,\n",
    "                            additional_forward_args=(data.edge_index),\n",
    "                            internal_batch_size=1)\n",
    "\n",
    "# Scale attributions to [0, 1]:\n",
    "ig_attr_node = ig_attr_node.squeeze(0).abs().sum(dim=1)\n",
    "ig_attr_node /= ig_attr_node.max()\n",
    "\n",
    "# Visualize absolute values of attributions:\n",
    "ax, G = explainer.visualize_subgraph(output_idx, data.edge_index, ig_attr_edge,\n",
    "                                     node_alpha=ig_attr_node)\n",
    "plt.show()\n",
    "\n",
    "# Node and edge explainability\n",
    "# ============================\n",
    "\n",
    "captum_model = to_captum_model(model, mask_type='node_and_edge',\n",
    "                               output_idx=output_idx)\n",
    "\n",
    "ig = IntegratedGradients(captum_model)\n",
    "ig_attr_node, ig_attr_edge = ig.attribute(\n",
    "    (data.x.unsqueeze(0), edge_mask.unsqueeze(0)), target=target,\n",
    "    additional_forward_args=(data.edge_index), internal_batch_size=1)\n",
    "\n",
    "# Scale attributions to [0, 1]:\n",
    "ig_attr_node = ig_attr_node.squeeze(0).abs().sum(dim=1)\n",
    "ig_attr_node /= ig_attr_node.max()\n",
    "ig_attr_edge = ig_attr_edge.squeeze(0).abs()\n",
    "ig_attr_edge /= ig_attr_edge.max()\n",
    "\n",
    "# Visualize absolute values of attributions:\n",
    "ax, G = explainer.visualize_subgraph(output_idx, data.edge_index, ig_attr_edge,\n",
    "                                     node_alpha=ig_attr_node)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

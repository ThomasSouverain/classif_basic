{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Hybrinfox - XGBoost Tutorial on Census - 2023-07\n",
    "\n",
    "Basic data preparation, training with XGBoost and analysis for binary classification (Census).\n",
    "\n",
    "Based on individuals with information on their age, occupation, relationship, sex... The purpose is to predict which one earns over $50_000 a year.  \n",
    "\n",
    "The dataset we used is the [Adult Census Income](https://www.kaggle.com/datasets/uciml/adult-census-income) dataset, made up of 58_000 census records on American citizens in 1994.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e65e0",
   "metadata": {},
   "source": [
    "# Pre-processing of data to feed the XGBoost model\n",
    "\n",
    "We begin with the required imports for data preparation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a741869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset on clients for binary classification\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d206c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447f8c",
   "metadata": {},
   "source": [
    "Once the data are loaded, split them in train / valid / test samples. \n",
    "- During training, the XGBoost model will use the train set to characterize, and the valid set to generalize the correlations between clients (cross-validation)\n",
    "- The test sample will only be used to assess if the final model generalizes well the clients' patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e01c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classif_basic.data_preparation import train_valid_test_split\n",
    "\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "model_task = \"classification\"\n",
    "xgb_eval_metric=\"auc\"\n",
    "\n",
    "X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test = train_valid_test_split(\n",
    "    X=X,\n",
    "    Y=Y, \n",
    "    model_task=model_task,\n",
    "    preprocessing_cat_features=preprocessing_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e26fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112fef30",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We begin with setting the hyperparameters (which need to be fine-tuned by the data-scientist):\n",
    "\n",
    "There are the [most frequently tuned hyperparameters](https://www.kaggle.com/code/soheiltehranipour/xgboost-tutorial-classification):\n",
    "\n",
    "1. **learning_rate**\n",
    "\n",
    "Also called eta, it specifies how quickly the model fits the residual errors by using additional base learners.\n",
    "\n",
    "Typical values: 0.01–0.2\n",
    "\n",
    "2. **gamma, reg_alpha, reg_lambda**\n",
    "\n",
    "These 3 parameters specify the values for 3 types of regularization (to make the model lighter) done by XGBoost - minimum loss reduction to create a new split, L1 reg on leaf weights, L2 reg leaf weights respectively\n",
    "\n",
    "Typical values for gamma: 0 - 0.5 but highly dependent on the data \n",
    "\n",
    "Typical values for reg_alpha and reg_lambda: 0 - 1 is a good starting point but again, depends on the data\n",
    "\n",
    "3. **max_depth**\n",
    "\n",
    "How deep the tree's decision nodes can go (maximal number of splitting features). A high number enhanced the results on training set, but can also lead to overfitting - as slow computation. Must be a positive integer. \n",
    "\n",
    "Typical values: 1–10\n",
    "\n",
    "4. **subsample** \n",
    "\n",
    "Fraction of the training set that can be used to train each tree. If this value is low, it may lead to underfitting or if it is too high, it may lead to overfitting.\n",
    "\n",
    "Typical values: 0.5–0.9\n",
    "\n",
    "5. **colsample_bytree** \n",
    "\n",
    "Fraction of the features that can be used to train each tree. A large value means almost all features can be used to build each decision tree\n",
    "\n",
    "Typical values: 0.5–0.9\n",
    "\n",
    "6. **seed**\n",
    "\n",
    "To grant that trainings of every model with the same hyper-parameters will provide similar results - splittings will be done by XGBoost according to the same individuals.\n",
    "\n",
    "Typical values: random integer, must only be specified\n",
    "\n",
    "7. **n_estimators**\n",
    "\n",
    "Maximal number of decision trees (weak learners) which can be aggregated in the XGBoost model. The generation of new trees is interrupted when the results on train-valid sets no more improve (cross-validation). \n",
    "\n",
    "... Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "\n",
    "gamma=0.2 \n",
    "reg_alpha=0.7 \n",
    "reg_lambda=0.8\n",
    "\n",
    "max_depth=6\n",
    "\n",
    "subsample=0.9\n",
    "\n",
    "seed=7\n",
    "\n",
    "n_estimators=1000\n",
    "\n",
    "xgb_classif_params = {\n",
    "    \"learning_rate\":learning_rate,\n",
    "    \"gamma\":gamma,\n",
    "    \"reg_alpha\":reg_alpha,\n",
    "    \"reg_lambda\":reg_lambda,\n",
    "    \"max_depth\":max_depth,\n",
    "    \"subsample\":subsample,\n",
    "    \"seed\":seed,\n",
    "    \"n_estimators\":n_estimators,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"importance_type\": \"gain\",\n",
    "    \"use_label_encoder\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf530f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "model = xgboost.XGBClassifier(**xgb_classif_params)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    eval_metric=xgb_eval_metric,\n",
    "    early_stopping_rounds=100, # 10\n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "    verbose=100,\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"\\n Training of the XGBoost model took {round(t1-t0)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a51413",
   "metadata": {},
   "source": [
    "# Evaluating the statistical performance of the new classifier\n",
    "\n",
    "We note that this evaluation is highly dependent on the performance indicator set by the user (e.g. ROC-AUC)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a2673",
   "metadata": {},
   "source": [
    "First, we must switch from probabilities to binary classification (here, does the individual earn over $50_000?). We choose a threshold maximising the F1 score (measure of the balance between precision and recall) on the **validation set** to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c682677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classif_basic.model import compute_best_fscore\n",
    "\n",
    "proba_valid = model.predict_proba(X_valid)[:, 1]\n",
    "proba_train_valid = model.predict_proba(X_train_valid)[:, 1]\n",
    "\n",
    "## set y predicted with optimised thresholds\n",
    "best_threshold, best_fscore = compute_best_fscore(Y_valid, proba_valid)\n",
    "\n",
    "Y_pred_train_valid = (proba_train_valid >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e7127",
   "metadata": {},
   "source": [
    "Now, we compare the performances of the XGBoost model on train, valid and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report_train_valid = classification_report(y_true=Y_train_valid, y_pred=Y_pred_train_valid)\n",
    "\n",
    "print(f\"Statistical Performance on Train & Valid Samples \\n \\n {report_train_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7129d",
   "metadata": {},
   "source": [
    "Do the same on the test sample, to inspect if the XGBoost model generalizes well on unseen clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f15ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_test = model.predict_proba(X_test)[:, 1]\n",
    "Y_pred_test = (proba_test >= best_threshold).astype(int)\n",
    "\n",
    "report_test = classification_report(y_true=Y_test, y_pred=Y_pred_test)\n",
    "\n",
    "print(f\"Statistical Performance on Test Samples \\n \\n {report_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

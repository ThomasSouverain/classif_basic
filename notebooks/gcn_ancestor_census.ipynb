{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83432a89",
   "metadata": {},
   "source": [
    "# GCN on Census\n",
    "There is an example of using a GCN on a tabular dataset for binary classification (here, Census to detect the people earning > $50_000). We suppose we already have some **logically consistent arrows** (coming from logical analysis of data -> all the coherent DAGs), that we want the GCN to learn - **phase 2** . \n",
    "\n",
    "**Causal hierarchy** could be introduced in the [definition of neighbors](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/loader/neighbor_loader.html#NeighborLoader) to build the subgraphs (i.e. batches of DataLoader)? \n",
    "\n",
    "Maybe: to specify the different \"relations\", we need to build a [heterogeneous graph](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/to_hetero_mag.py)? \n",
    "Begin with these constraints:\n",
    "    - Graph data 1: edge \"sex\"\n",
    "    - Graph data 2: edge \"work -> hours of work\"\n",
    "    - Graph data 1 -> (inherits from; temporal?) Graph data 2\n",
    "\n",
    "In this notebook, we inspect **in which way a tabular dataset as Census can be used by an AI based on graphs to estimate wealthiness of individuals**. \n",
    "\n",
    "Therefore, we proceed in 2 steps:\n",
    "\n",
    "**1. We prepare data to be handled by a model based on a graph**\n",
    "We transform them into a graph, that involves strong assumptions on the features involved in connections...\n",
    "\n",
    "**2. We train an AI based on graphs**\n",
    "Here, we begin with a Graphical Neural Network (GNN) based on a Multi-Layer Perceptron (MLP), requiring the library Torch.\n",
    "\n",
    "**3. We inspect if the graph-based AI indeed reflects common & expert knowledge on**\n",
    "In particular, regarding the non-sense of certain inferences that should absolutely be avoided (e.g. education may influence occupation, but not the reverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc841c5f",
   "metadata": {},
   "source": [
    "## Enforcing causal paths in tabular GNN - Full data-graph (x, edge_index) child knowledge\n",
    "\n",
    "To train a GNN while respecting a minimal set of causal paths, we pass to the GNN 2 types of graph-data:\n",
    "- ancestor: with only the ancestor nodes\n",
    "- child: ancestor + child nodes, adding as an edge \"ancestor -> child\"\n",
    "\n",
    "-> s.t. child(n) becomes the ancestor of child(n+1)\n",
    "\n",
    "Constitute 2 graph-data parent/child, suggesting causality by adding child nodes (and also edge: parent -> child) in the child data.\n",
    "\n",
    "For the moment, we specify only 1 parent per edge, on 2 layers:\n",
    "- ancestor layer: age -> occupation\n",
    "- child layer: occupation -> hours of work per week\n",
    "\n",
    "For the moment, to avoid spurious correlations we also keep only the ancestor features (age, sex, race, native country) as node features for all graph-data. Based on this ancestor \"blind knowledge\", add the child as node features (and also edge: parent -> child) in the child graph-data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc5f3b",
   "metadata": {},
   "source": [
    "Causal analysis: before train&valid/test split, we reduced the number of features to contain only \"straightforward\" causal information -> enabling to integrate it progressively in our GNN (through edges). Therefore, we use factor analysis:\n",
    "\n",
    "To control for the balance of df across classes, we sort the clients so that X gets perfect equality in the repartition of classes 0 and 1 (at the cost of 25 000 instead of the 45 000 initial individuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19753f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports and train/test split (to be put in part 2. of the notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    TORCH = torch.__version__.split(\"+\")[0]\n",
    "    CUDA = \"cu\" + torch.version.cuda.replace(\".\",\"\")\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "#!pip install torch-geometric\n",
    "#import torch_geometric\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from classif_basic.data_preparation import handle_cat_features\n",
    "\n",
    "from classif_basic.graph.data_to_graph import table_to_graph, add_new_edge\n",
    "from classif_basic.graph.train import train_GNN_ancestor\n",
    "\n",
    "# preparing the dataset on clients for binary classification\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1\n",
    "\n",
    "SEED = 7\n",
    "VALID_SIZE = 0.15\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "X = handle_cat_features(X=X, preprocessing_cat_features=preprocessing_cat_features)\n",
    "\n",
    "# first of all, unify features with \"redundant\" causal information\n",
    "from classif_basic.graph.utils import get_unified_col\n",
    "\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"education\",\"education-num\"], new_col_name = \"education\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"relationship\",\"marital-status\"], new_col_name = \"relationship\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"occupation\",\"workclass\"], new_col_name = \"job\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"capital-gain\",\"capital-loss\"], new_col_name = \"capital\")\n",
    "\n",
    "# select equal proportion of classes \"wealthy\" and \"not wealthy\", and generates the new dataset accordingly\n",
    "from classif_basic.graph.utils import get_balanced_df\n",
    "\n",
    "balanced_df = get_balanced_df(X=X, Y=Y)\n",
    "\n",
    "X_balanced = balanced_df.drop(\"target\", axis=1)\n",
    "Y_balanced = balanced_df[\"target\"]\n",
    "\n",
    "X=X_balanced # here, we try with the whole dataset (assuming it is imbalanced, but counts almost 50 000 nodes)\n",
    "Y=Y_balanced\n",
    "\n",
    "# then, normalize the df categories for better neural-network computation\n",
    "from classif_basic.graph.utils import normalize_df\n",
    "\n",
    "X=normalize_df(df=X, normalization='min_max')\n",
    "\n",
    "# Split valid set for early stopping & model selection\n",
    "# \"stratify=Y\" to keep the same proportion of target classes in train/valid (i.e. model) and test sets \n",
    "X_model, X_test, Y_model, Y_test = train_test_split(\n",
    "    X, Y, test_size=VALID_SIZE, random_state=SEED, stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd82f7",
   "metadata": {},
   "source": [
    "# Data to ancestor & child Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5638622",
   "metadata": {},
   "source": [
    "Cascade de causes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load already formed graph-data, to gain time\n",
    "from classif_basic.model import pickle_load_model\n",
    "\n",
    "dict_data_total = pickle_load_model(\"/work/data/graph_data/balanced/dict_all_edges.pkl\")\n",
    "\n",
    "data_full_education_relationship = pickle_load_model(\"/work/data/graph_data/balanced/data_full_features_education_relationship.pkl\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4535e348",
   "metadata": {},
   "source": [
    "# or form other types of graph-data (edges) \n",
    "\n",
    "from classif_basic.graph.data_to_graph import get_parent_child_data\n",
    "\n",
    "list_data_total = []\n",
    "dict_data_total = {}\n",
    "\n",
    "list_first_ancestors = ['race', 'sex', 'native-country', 'age']\n",
    "edge_parent = \"fnlwgt\"\n",
    "edge_child0 = \"education\" \n",
    "edge_child1 = \"relationship\"\n",
    "edge_child2 = \"job\"\n",
    "edge_child3 = \"hours-per-week\"\n",
    "edge_child4 = \"capital\"\n",
    "\n",
    "list_successive_paths=[\"fnlwgt\", \"education\" , \"relationship\", \"job\", \"hours-per-week\"]#, \"capital\"]\n",
    "\n",
    "for i in range(len(list_successive_paths)-1):\n",
    "    edge_parent = list_successive_paths[i]\n",
    "    edge_child = list_successive_paths[i+1]\n",
    "    print(f\"\\n {edge_parent} -> {edge_child}\")\n",
    "    data_total = get_parent_child_data(X=X_model, Y=Y_model, list_node_features=list_first_ancestors, \n",
    "                                                edge_parent=edge_parent, edge_child=edge_child)\n",
    "    list_data_total.append(data_total)\n",
    "    dict_data_total[f\"{edge_parent}->{edge_child}\"] = data_total\n",
    "    print(data_total)\n",
    "   \n",
    "# also load a graph-data with all the features and only one edge, for comparison with a \"classic\" GNN\n",
    "data_full_education_relationship = get_parent_child_data(X=X_model, Y=Y_model, list_node_features=X_model.columns, \n",
    "                                                edge_parent=\"education\", edge_child=\"relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b5cd3",
   "metadata": {},
   "source": [
    "# Train a basic Graph Neural Network on the graph-shaped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cde5a",
   "metadata": {},
   "source": [
    "## Train with batches (neighborhood sampling) a basic GCN \n",
    "\n",
    "Here, we try using the batches constituted from neighborhoods to train the GNN, using our GPU (if accessed).\n",
    "\n",
    "We use our GCN_ancestor class progressively adding through layers the \"causal child\" information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc608",
   "metadata": {},
   "source": [
    "Here with batches of 128 individuals, 76% of accuracy is reached by passing a causal order on layer1 and layer2 (accuracy == to the situation where all features are specified, and no causal layer!)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd2d57",
   "metadata": {},
   "source": [
    "We get here our own data-loader, ensuring that each batch passes the same individuals to the GNN (s.t. only causal data changes through layers):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441f046",
   "metadata": {},
   "source": [
    "We begin with all successive 3 edges (without fnlwgt, not enough info in edge_index?) given in our (basic) GCN with successive causal layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052f652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loader_method=\"index_groups\"\n",
    "model_type=\"conv\"\n",
    "loss_name=\"CrossEntropyLoss\"\n",
    "#batch_size=10_000#19_868\n",
    "learning_rate = 0.01\n",
    "nb_batches=1\n",
    "epoch_nb = 1000\n",
    "cv_step=100\n",
    "\n",
    "skip_connection=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cef7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# don't forget to reallocate GPU memory, before new training!\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# and to build the index, pass the data-graph to CPU device\n",
    "dict_data_total['education->relationship'].cpu()\n",
    "\n",
    "# combine the 2 edges? Arriving respectively to 0.4 and 0.3 of losses\n",
    "gnn_education_job = train_GNN_ancestor(\n",
    "                list_data_total=[dict_data_total['education->relationship'],\n",
    "                                 dict_data_total['relationship->job']],\n",
    "                model_type=model_type,\n",
    "                loader_method=loader_method,\n",
    "                loss_name=loss_name,\n",
    "                #batch_size=batch_size,\n",
    "                nb_batches=nb_batches,\n",
    "                epoch_nb = epoch_nb,\n",
    "                cv_step=cv_step,\n",
    "                learning_rate = learning_rate,\n",
    "                skip_connection=skip_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995faf1",
   "metadata": {},
   "source": [
    "## Standard GCN on all features for comparison\n",
    "Finally, we compare these GCNs trained with progressively incorporated / partial edges, with a GCN trained with all features as node features (correlating them...) and only the edge \"job -> work hours\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e907374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gnn_classic = train_GNN_ancestor(\n",
    "                list_data_total=[data_full_education_relationship],\n",
    "                model_type=model_type,\n",
    "                loader_method=loader_method,\n",
    "                loss_name=loss_name,\n",
    "                #batch_size=batch_size,\n",
    "                nb_batches=nb_batches,\n",
    "                epoch_nb = epoch_nb,\n",
    "                cv_step=cv_step,\n",
    "                learning_rate = learning_rate,\n",
    "                skip_connection=skip_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8c380",
   "metadata": {},
   "source": [
    "# Basic XGB for comparison - excellent results\n",
    "With the same unified features 92% ROC-AUC, 81-84% PR-AUC, 85% accuracy on train&valid sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e23889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classif_basic.graph.train import train_xgb_benchmark\n",
    "\n",
    "train_xgb_benchmark(X_train=X_train, X_valid=X_valid, Y_train=Y_train, Y_valid=Y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc841c5f",
   "metadata": {},
   "source": [
    "## Enforcing causal paths in tabular GNN - Full data-graph (x, edge_index) child knowledge\n",
    "\n",
    "To train a GNN while respecting a minimal set of causal paths, we pass to the GNN 2 types of graph-data:\n",
    "- ancestor: with only the ancestor nodes\n",
    "- child: ancestor + child nodes, adding as an edge \"ancestor -> child\"\n",
    "\n",
    "-> s.t. child(n) becomes the ancestor of child(n+1)\n",
    "\n",
    "Constitute 2 graph-data parent/child, suggesting causality by adding child nodes (and also edge: parent -> child) in the child data.\n",
    "\n",
    "For the moment, we specify only 1 parent per edge, on 2 layers:\n",
    "- ancestor layer: age -> occupation\n",
    "- child layer: occupation -> hours of work per week\n",
    "\n",
    "For the moment, to avoid spurious correlations we also keep only the ancestor features (age, sex, race, native country) as node features for all graph-data. Based on this ancestor \"blind knowledge\", add the child as node features (and also edge: parent -> child) in the child graph-data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc5f3b",
   "metadata": {},
   "source": [
    "Causal analysis: before train&valid/test split, we reduced the number of features to contain only \"straightforward\" causal information -> enabling to integrate it progressively in our GNN (through edges). Therefore, we use factor analysis:\n",
    "\n",
    "To control for the balance of df across classes, we sort the clients so that X gets perfect equality in the repartition of classes 0 and 1 (at the cost of 25 000 instead of the 45 000 initial individuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19753f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports and train/test split (to be put in part 2. of the notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    TORCH = torch.__version__.split(\"+\")[0]\n",
    "    CUDA = \"cu\" + torch.version.cuda.replace(\".\",\"\")\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "#!pip install torch-geometric\n",
    "#import torch_geometric\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from classif_basic.data_preparation import handle_cat_features\n",
    "\n",
    "from classif_basic.graph.data_to_graph import table_to_graph, add_new_edge\n",
    "from classif_basic.graph.train import train_GNN_ancestor\n",
    "\n",
    "# preparing the dataset on clients for binary classification\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1\n",
    "\n",
    "SEED = 7\n",
    "VALID_SIZE = 0.15\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "X = handle_cat_features(X=X, preprocessing_cat_features=preprocessing_cat_features)\n",
    "\n",
    "# select equal proportion of classes \"wealthy\" and \"not wealthy\", and generates the new dataset accordingly\n",
    "from classif_basic.graph.utils import get_balanced_df\n",
    "\n",
    "balanced_df = get_balanced_df(X=X, Y=Y)\n",
    "\n",
    "X_balanced = balanced_df.drop(\"target\", axis=1)\n",
    "Y_total_balanced = balanced_df[\"target\"]\n",
    "\n",
    "#X=X_balanced # here, we try with the whole dataset (assuming it is imbalanced, but counts almost 50 000 nodes)\n",
    "#Y=Y_balanced\n",
    "\n",
    "# first of all, unify features with \"redundant\" causal information\n",
    "from classif_basic.graph.utils import get_unified_col\n",
    "\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"education\",\"education-num\"], new_col_name = \"education\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"relationship\",\"marital-status\"], new_col_name = \"relationship\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"occupation\",\"workclass\"], new_col_name = \"job\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"capital-gain\",\"capital-loss\"], new_col_name = \"capital\")\n",
    "\n",
    "# Split valid set for early stopping & model selection\n",
    "# \"stratify=Y\" to keep the same proportion of target classes in train/valid (i.e. model) and test sets \n",
    "X_model, X_test, Y_model, Y_test = train_test_split(\n",
    "    X, Y, test_size=VALID_SIZE, random_state=SEED, stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd82f7",
   "metadata": {},
   "source": [
    "# Data to ancestor & child Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5638622",
   "metadata": {},
   "source": [
    "Cascade de causes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classif_basic.graph.data_to_graph import get_parent_child_data\n",
    "\n",
    "list_data_total = []\n",
    "\n",
    "list_first_ancestors = ['race', 'sex', 'native-country', 'age']\n",
    "edge_parent = \"fnlwgt\"\n",
    "edge_child0 = \"education\" \n",
    "edge_child1 = \"relationship\"\n",
    "edge_child2 = \"job\"\n",
    "edge_child3 = \"hours-per-week\"\n",
    "edge_child4 = \"capital\"\n",
    "\n",
    "list_successive_paths=[\"education\" , \"job\", \"hours-per-week\"]\n",
    "\n",
    "for i in range(len(list_successive_paths)-1):\n",
    "    edge_parent = list_successive_paths[i]\n",
    "    edge_child = list_successive_paths[i+1]\n",
    "    print(f\"\\n {edge_parent} -> {edge_child}\")\n",
    "    data_total = get_parent_child_data(X=X_model, Y=Y_model, list_node_features=list_first_ancestors, \n",
    "                                                edge_parent=edge_parent, edge_child=edge_child1)\n",
    "    list_data_total.append(data_total)\n",
    "    print(data_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b5cd3",
   "metadata": {},
   "source": [
    "# Train a basic Graph Neural Network on the graph-shaped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cde5a",
   "metadata": {},
   "source": [
    "## Train with batches (neighborhood sampling) a basic GCN \n",
    "\n",
    "Here, we try using the batches constituted from neighborhoods to train the GNN, using our GPU (if accessed).\n",
    "\n",
    "We use our GCN_ancestor class progressively adding through layers the \"causal child\" information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc608",
   "metadata": {},
   "source": [
    "Here with batches of 128 individuals, 76% of accuracy is reached by passing a causal order on layer1 and layer2 (accuracy == to the situation where all features are specified, and no causal layer!)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd2d57",
   "metadata": {},
   "source": [
    "We get here our own data-loader, ensuring that each batch passes the same individuals to the GNN (s.t. only causal data changes through layers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_job_hours = get_parent_child_data(X=X_model, Y=Y_model, list_node_features=X_model.columns, \n",
    "                                                edge_parent=\"job\", edge_child=\"hours-per-week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052f652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with the method \"index_groups\": with 300 batches and 2 epochs, 70%(epoch1) -> 76% of accuracy (5 mn)\n",
    "# ||| Epoch 2 Loss_train = 1.1 Loss_valid = 0.55 Train & Valid Accuracy = 0.76\n",
    "\n",
    "# here with skip connections\n",
    "\n",
    "#list_data_total = [data_total_child1, data_total_child2, data_total_child3]\n",
    "loader_method=\"index_groups\"\n",
    "\n",
    "model_type=\"conv_attention\"\n",
    "loss_name=\"CrossEntropyLoss\"\n",
    "batch_size=10_000#19_868\n",
    "learning_rate = 0.01\n",
    "#nb_batches=100\n",
    "epoch_nb = 50\n",
    "\n",
    "batch_size=25_000\n",
    "\n",
    "gnn_neighbor = train_GNN_ancestor(\n",
    "                list_data_total=[data_job_hours],\n",
    "                model_type=model_type,\n",
    "                loader_method=loader_method,\n",
    "                loss_name=loss_name,\n",
    "                batch_size=batch_size,\n",
    "                #nb_batches=nb_batches,\n",
    "                epoch_nb = epoch_nb,\n",
    "                learning_rate = learning_rate,\n",
    "                skip_connection=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e176c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# currently working, with the method \"neighbor_nodes\"\n",
    "# 17 mn with 300 epochs (Epoch 300 Loss_train = 0.55 Loss_valid = 0.28 Train & Valid Accuracy = 0.76)\n",
    "# but: not the same individuals sampled across the layers! Take the neighbours of data-ancestor -> keep index? \n",
    "\n",
    "# list_data_total = [data_total_child1, data_total_child2, data_total_child3]\n",
    "loader_method=\"neighbor_nodes\"\n",
    "\n",
    "epoch_nb = 5\n",
    "\n",
    "gnn_neighbor = train_GNN_ancestor(\n",
    "                list_data_total=list_data_total,\n",
    "                model_type=model_type,\n",
    "                loader_method=loader_method,\n",
    "                loss_name=loss_name,\n",
    "                batch_size=batch_size,\n",
    "                #nb_batches=nb_batches,\n",
    "                epoch_nb = epoch_nb,\n",
    "                learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995faf1",
   "metadata": {},
   "source": [
    "## Standard GCN on all features for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1abde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GATConv(13,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4779aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.conv import GATConv\n",
    "\n",
    "data = list_data_total[-1]\n",
    "\n",
    "conv = GATConv(data.num_features, 32)\n",
    "conv(data.x.float(), data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_one_data[0].num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e907374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a GNN with the unique edge \"education -> job\"\n",
    "#6 features as well, but same data.x => our assumptions: the AUCs will remain low, \n",
    "# but not as fluctuating as with 2 graphs using different neighbor sampling (and 2 different x in layers)\n",
    "\n",
    "list_one_data = [list_data_total[-1]] # only trained with ancestors (x) and one edge (fnlwgt->education)\n",
    "loader_method=\"neighbor_nodes\"\n",
    "loss_name=\"CrossEntropyLoss\" \n",
    "\n",
    "unique_data_graph=True\n",
    "\n",
    "epoch_nb = 5\n",
    "batch_size=25_000\n",
    "\n",
    "gnn_neighbor = train_GNN_ancestor(\n",
    "                list_data_total=list_one_data,\n",
    "                model_type=model_type,\n",
    "                loader_method=loader_method,\n",
    "                loss_name=loss_name,\n",
    "                batch_size=batch_size,\n",
    "                #nb_batches=nb_batches,\n",
    "                epoch_nb = epoch_nb,\n",
    "                learning_rate = learning_rate,\n",
    "                unique_data_graph=unique_data_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8c380",
   "metadata": {},
   "source": [
    "# Basic XGB for comparison - excellent results\n",
    "With the same unified features 92% ROC-AUC, 81-84% PR-AUC, 85% accuracy on train&valid sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classif_basic.data_preparation import train_valid_test_split\n",
    "from classif_basic.model import train_naive_xgb\n",
    "\n",
    "model_task=\"classification\"\n",
    "stat_criteria=\"aucpr\"\n",
    "\n",
    "X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test=train_valid_test_split(\n",
    "    X=X,\n",
    "    Y=Y, \n",
    "    model_task=model_task, \n",
    "    preprocessing_cat_features=preprocessing_cat_features)\n",
    "\n",
    "Y_pred_train_valid = train_naive_xgb(\n",
    "    X_train=X_train,\n",
    "    X_valid=X_valid,\n",
    "    X_train_valid=X_train_valid,\n",
    "    X_test=X_test,\n",
    "    Y_train=Y_train,\n",
    "    Y_valid=Y_valid,\n",
    "    Y_train_valid=Y_train_valid,\n",
    "    Y_test=Y_test,\n",
    "    model_task=model_task,\n",
    "    stat_criteria=stat_criteria,\n",
    ") \n",
    "\n",
    "total_target = Y_train_valid.shape[0]\n",
    "total_exact=(Y_pred_train_valid==Y_train_valid).sum()#.all()\n",
    "\n",
    "xgb_accuracy = total_exact/total_target\n",
    "xgb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc053d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

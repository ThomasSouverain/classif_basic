{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc841c5f",
   "metadata": {},
   "source": [
    "## Enforcing causal paths in tabular GNN - Full data-graph (x, edge_index) child knowledge\n",
    "\n",
    "To train a GNN while respecting a minimal set of causal paths, we pass to the GNN 2 types of graph-data:\n",
    "- ancestor: with only the ancestor nodes\n",
    "- child: ancestor + child nodes, adding as an edge \"ancestor -> child\"\n",
    "\n",
    "-> s.t. child(n) becomes the ancestor of child(n+1)\n",
    "\n",
    "Constitute 2 graph-data parent/child, suggesting causality by adding child nodes (and also edge: parent -> child) in the child data.\n",
    "\n",
    "For the moment, we specify only 1 parent per edge, on 2 layers:\n",
    "- ancestor layer: age -> occupation\n",
    "- child layer: occupation -> hours of work per week\n",
    "\n",
    "For the moment, to avoid spurious correlations we also keep only the ancestor features (age, sex, race, native country) as node features for all graph-data. Based on this ancestor \"blind knowledge\", add the child as node features (and also edge: parent -> child) in the child graph-data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19753f63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# imports and train/test split (to be put in part 2. of the notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    TORCH = torch.__version__.split(\"+\")[0]\n",
    "    CUDA = \"cu\" + torch.version.cuda.replace(\".\",\"\")\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "#!pip install torch-geometric\n",
    "#import torch_geometric\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from classif_basic.data_preparation import handle_cat_features\n",
    "\n",
    "from classif_basic.graph.data_to_graph import table_to_graph, add_new_edge\n",
    "from classif_basic.graph.train import train_GNN_ancestor\n",
    "\n",
    "# preparing the dataset on clients for binary classification\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1\n",
    "\n",
    "SEED = 7\n",
    "VALID_SIZE = 0.15\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "X = handle_cat_features(X=X, preprocessing_cat_features=preprocessing_cat_features)\n",
    "\n",
    "# Split valid set for early stopping & model selection\n",
    "# \"stratify=Y\" to keep the same proportion of target classes in train/valid (i.e. model) and test sets \n",
    "X_model, X_test, Y_model, Y_test = train_test_split(\n",
    "    X, Y, test_size=VALID_SIZE, random_state=SEED, stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19c2de",
   "metadata": {},
   "source": [
    "# Data to ancestor & child Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b137533",
   "metadata": {},
   "source": [
    "We begin with all parent features as nodes, and the directed edge parent->child1 (here, age->job): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total=X_model\n",
    "Y_total=Y_model\n",
    "\n",
    "median_age = X_total[\"age\"].median() # merge 'age' in 2 age categories to form the edges faster\n",
    "\n",
    "X_total[\"age\"] = (X_total[\"age\"] == median_age).astype(int)\n",
    "\n",
    "list_child1 = [\"race\", \"sex\", \"native-country\", \"age\", \"occupation\"]\n",
    "edge_parent = \"age\"\n",
    "edge_child1 = \"occupation\"\n",
    "\n",
    "X_total_child1 = X_total.filter(list_child1)\n",
    "\n",
    "# add \"sex\" as a connection (edge) between these ancestors features\n",
    "edges_total_child1 = add_new_edge(data=X_total_child1, previous_edge=None, \n",
    "                                list_col_names=[edge_parent, edge_child1])\n",
    "\n",
    "# being edges, \"sex\" and \"education\" and must be removed from the nodes\n",
    "list_child1.remove(edge_parent)\n",
    "list_child1.remove(edge_child1)\n",
    "\n",
    "data_total_child1 = table_to_graph(X=X_total_child1, Y=Y_total, list_col_names=list_child1, edges=edges_total_child1)\n",
    "print(f\"data_total_child1: {data_total_child1} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d29a1c",
   "metadata": {},
   "source": [
    "Then, we add the descendant node feature \"hours of work per week\" (new graph data), and the new directed edge child1->child2 (here, job -> hours of work per week):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0364cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW GRAPH-DATA integrating child2 (to parent & child1 data)\n",
    "median_hours = X_total[\"hours-per-week\"].median() # '1' if the client works over 40 hours per week\n",
    "\n",
    "X_total[\"hours-per-week\"] = (X_total[\"hours-per-week\"] == median_hours).astype(int)\n",
    "\n",
    "list_child2 = [\"race\", \"sex\", \"native-country\", \"age\", \"occupation\", \"hours-per-week\"]\n",
    "edge_parent = \"age\"\n",
    "edge_child1 = \"occupation\"\n",
    "edge_child2 = \"hours-per-week\"\n",
    "\n",
    "X_total_child2 = X_total.filter(list_child2)\n",
    "\n",
    "# add \"sex\" as a connection (edge) between these ancestors features\n",
    "edges_total_child2 = add_new_edge(data=X_total_child2, previous_edge=None, \n",
    "                                list_col_names=[edge_child1, edge_child2])\n",
    "\n",
    "# being edges, \"occupation\" and \"hours-per-week\" must be removed from the nodes\n",
    "# list_child2.remove(edge_parent) -> TODO remove edge_parent, to avoid correlation vs causation? It seems to me not necessary\n",
    "list_child2.remove(edge_child1)\n",
    "list_child2.remove(edge_child2)\n",
    "\n",
    "data_total_child2 = table_to_graph(X=X_total_child2, Y=Y_total, list_col_names=list_child2, edges=edges_total_child2)\n",
    "print(f\"data_total_child2: {data_total_child2} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b5cd3",
   "metadata": {},
   "source": [
    "# Train a basic Graph Neural Network on the graph-shaped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2cde5a",
   "metadata": {},
   "source": [
    "## Train with batches (neighborhood sampling) a basic GCN \n",
    "\n",
    "Here, we try using the batches constituted from neighborhoods to train the GNN, using our GPU (if accessed).\n",
    "\n",
    "We use our GCN_ancestor class progressively adding through layers the \"causal child\" information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc608",
   "metadata": {},
   "source": [
    "Here with batches of 128 individuals, 76% of accuracy is reached by passing a causal order on layer1 and layer2 (accuracy == to the situation where all features are specified, and no causal layer!)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd2d57",
   "metadata": {},
   "source": [
    "We get here our own data-loader, ensuring that each batch passes the same individuals to the GNN (s.t. only causal data changes through layers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9ac93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with the method \"index_groups\": with 300 batches and 2 epochs, 70%(epoch1) -> 76% of accuracy (5 mn)\n",
    "# ||| Epoch 2 Loss_train = 1.1 Loss_valid = 0.55 Train & Valid Accuracy = 0.76\n",
    "\n",
    "list_data_total = [data_total_child1, data_total_child2]\n",
    "loader_method=\"index_groups\"\n",
    "loss_name=\"CrossEntropyLoss\"\n",
    "#batch_size=150\n",
    "nb_batches=300\n",
    "epoch_nb = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "gnn_index = train_GNN_ancestor(\n",
    "                list_data_total=list_data_total,\n",
    "                loader_method=loader_method,\n",
    "                loss_name=loss_name,\n",
    "                #batch_size=batch_size,\n",
    "                nb_batches=nb_batches,\n",
    "                epoch_nb = epoch_nb,\n",
    "                learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e176c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# currently working, with the method \"neighbor_nodes\"\n",
    "# 17 mn with 300 epochs (Epoch 300 Loss_train = 0.55 Loss_valid = 0.28 Train & Valid Accuracy = 0.76)\n",
    "# but: not the same individuals sampled across the layers! Take the neighbours of data-ancestor -> keep index? \n",
    "\n",
    "list_data_total = [data_total_child1, data_total_child2]\n",
    "loader_method=\"neighbor_nodes\"\n",
    "loss_name=\"CrossEntropyLoss\" \n",
    "# need to retropropagate the gradient => should not detach it! Instead, compute AUC with gradient present?\n",
    "\n",
    "#batch_size=150\n",
    "nb_batches=100\n",
    "epoch_nb = 30\n",
    "learning_rate = 0.01\n",
    "\n",
    "gnn_neighbor = train_GNN_ancestor(\n",
    "                list_data_total=list_data_total,\n",
    "                loader_method=loader_method,\n",
    "                loss_name=loss_name,\n",
    "                #batch_size=batch_size,\n",
    "                nb_batches=nb_batches,\n",
    "                epoch_nb = epoch_nb,\n",
    "                learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32385c",
   "metadata": {},
   "source": [
    "## Inspect the predictions of the model on valid and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b56e7a",
   "metadata": {},
   "source": [
    "Let's inspect the model on test data, to assess if the stability of performance is not due to coincidence:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a13d156",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from classif_basic.graph import evaluate_gnn\n",
    "\n",
    "classifier=gnn_basic\n",
    "data_test=data_test\n",
    "loss_name=\"cross_entropy\"\n",
    "\n",
    "# unfortunately, memory error... Evaluate per batches? Or create an independant data_test? // Evaluate on valid \n",
    "\n",
    "evaluate_gnn(\n",
    "    classifier=gnn_basic, \n",
    "    data_test=data_test, \n",
    "    loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f7e6a",
   "metadata": {},
   "source": [
    "**No Overfitting**\n",
    "\n",
    "With this very simple shape of graph-data (directed edge = \"job\" -> \"work hours\"), the accuracy remains 75% for train, valid and test data.\n",
    "\n",
    "It confirms us that the training through basic GNN, on basic shaped data, delivers here stable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

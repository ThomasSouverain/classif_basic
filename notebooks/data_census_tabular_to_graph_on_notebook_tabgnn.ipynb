{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2affb517",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Get a complete representation of connections in Census (with TabNN)?\n",
    "\n",
    "In this notebook, we inspect **in which way a tabular dataset as Census can be used by an AI based on graphs to estimate wealthiness of individuals**. \n",
    "\n",
    "Therefore, we proceed in 2 steps:\n",
    "\n",
    "**1. We prepare data to be handled by a model based on a graph**\n",
    "We transform them into a graph, that involves strong assumptions on the features involved in connections...\n",
    "\n",
    "**2. We train an AI based on graphs**\n",
    "Here, we begin with a Graphical Neural Network (GNN) based on a Multi-Layer Perceptron (MLP), requiring the library Torch.\n",
    "\n",
    "**3. We inspect if the graph-based AI indeed reflects common & expert knowledge on**\n",
    "In particular, regarding the non-sense of certain inferences that should absolutely be avoided (e.g. education may influence occupation, but not the reverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Data preparation for binary classification with graphs (Census)\n",
    "For this reshaping (and also interpretation, see below the choice of edges) of data tables to graphs, we based on the [Google tutorial](https://colab.research.google.com/drive/1_eR7DXBF3V4EwH946dDPOxeclDBeKNMD?usp=sharing#scrollTo=WuggdIItffpv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03693",
   "metadata": {},
   "source": [
    "## General preparation - handle categorical features\n",
    "Here, we handle the categorical features through label-encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from classif_basic.data_preparation import train_valid_test_split, set_target_if_feature, automatic_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124240e",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Fix precise % of population distribution (sex: Male, Female) and % of wealthiness according to sex. In that way, we could inspect if the structure of the model (here based on a graph) integrates this \"sexist\" representation of the world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a741869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset on clients for binary classification\n",
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbe273",
   "metadata": {},
   "source": [
    "For the moment, we exclude the 'sexist' bias to inspect how the data are linked, and further describe them through a (general, high-level) causal model..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "412ce254",
   "metadata": {},
   "source": [
    "dataset = X.copy()\n",
    "dataset['target'] = Y\n",
    "dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9412d5d9",
   "metadata": {},
   "source": [
    "# here, \"treatment\" is saw as being 'Male' and not 'Female'\n",
    "\n",
    "df_response_if_feature=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==1)]\n",
    "df_no_response_if_feature=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==0)]\n",
    "df_response_if_not_feature=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==1)]\n",
    "df_no_response_if_not_feature=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==0)]\n",
    "\n",
    "print(df_response_if_feature.shape[0])\n",
    "print(df_no_response_if_feature.shape[0])\n",
    "print(df_response_if_not_feature.shape[0])\n",
    "print(df_no_response_if_not_feature.shape[0])\n",
    "\n",
    "\n",
    "# % of men selected by the initial data\n",
    "df_response_if_feature.shape[0]/(df_response_if_feature.shape[0]+df_no_response_if_feature.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "584ed682",
   "metadata": {},
   "source": [
    "# % of women selected by the initial data\n",
    "df_response_if_not_feature.shape[0]/(df_response_if_feature.shape[0]+df_no_response_if_not_feature.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a34ce986",
   "metadata": {},
   "source": [
    "len_dataset = 20_000\n",
    "\n",
    "percentage_feature= 70\n",
    "percentage_response_if_feature=70\n",
    "percentage_response_if_not_feature=10\n",
    "\n",
    "sexist_dataset = set_target_if_feature(\n",
    "    df_response_if_feature=df_response_if_feature,\n",
    "    df_no_response_if_feature=df_no_response_if_feature,\n",
    "    df_response_if_not_feature=df_response_if_not_feature,\n",
    "    df_no_response_if_not_feature=df_no_response_if_not_feature,\n",
    "    len_dataset=len_dataset,\n",
    "    percentage_feature=percentage_feature,\n",
    "    percentage_response_if_feature=percentage_response_if_feature,\n",
    "    percentage_response_if_not_feature=percentage_response_if_not_feature)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "295a8b54",
   "metadata": {},
   "source": [
    "X = sexist_dataset.loc[: , dataset.columns != 'target']\n",
    "Y = sexist_dataset['target']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "852d76a4",
   "metadata": {},
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447f8c",
   "metadata": {},
   "source": [
    "### Train-test-split, to prepare for 3 graphs representing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e01c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task = \"classification\"\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test = train_valid_test_split(\n",
    "    X=X,\n",
    "    Y=Y, \n",
    "    model_task=model_task,\n",
    "    preprocessing_cat_features=preprocessing_cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99abfa",
   "metadata": {},
   "source": [
    "## Reshape (by interpreting) data to a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756db59",
   "metadata": {},
   "source": [
    "From this dataset (where we introduced selectively a \"sexist\" effect against women), let's see how we could swith from the tabular data to a graph representation. Here, we based on the [Google tutorial](https://colab.research.google.com/drive/1_eR7DXBF3V4EwH946dDPOxeclDBeKNMD?usp=sharing#scrollTo=Mw8dzPy3-UnJ) to switch from tables to graph. \n",
    "\n",
    "The point is that our features X all seem to be attributes of the clients, though we should find a way of representing their interactions between clients \n",
    "\n",
    "X = {race, age, sex, final weight (depends on age, sex, hispanic origin, race), education, education number, marital status, relationship, occupation, hours per week, workclass, race, sex, capital gain, capital loss, native country} \n",
    "\n",
    "**Nodes** \n",
    "Bank clients (by ID)\n",
    "\n",
    "**Edges** \n",
    "Here, we should find one or several ways of connecting the clients\n",
    "\n",
    "Should be occupation → if changes of occupation (or similar client with new occupation), which impact on the revenue? // change of football team => impact on the football rate \n",
    "(pers) actionable => predict revenue when switches to a new job??\n",
    "→ may be: “hours per week” <=> inspect the change of revenue if switches to greater hours per week?\n",
    "\n",
    "**Node Features** \n",
    "Attributs of the nodes, i.e. characteristics of the clients (here, hard to separate from what \"connects\" them...) \n",
    "\n",
    "Race, age, sex, final weight (depends on age, sex, hispanic origin, race), education, education number, marital status, relationship, hours per week, workclass, race, sex, capital gain, capital loss, native country \n",
    "\n",
    "**Label (here at a node-level?)** \n",
    "Income (Y = income > $50 000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first of all, specify the edge\n",
    "edge = \"occupation\"# str (for the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403774e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an idea of the codes corresponding to occupations, reconstituting labels' transformations from X\n",
    "le = LabelEncoder()\n",
    "\n",
    "dict_occupation_codes = pd.Series(X[edge].values, index=X.apply(le.fit_transform)[edge]).to_dict()\n",
    "\n",
    "# correct according to dict comparison\n",
    "dict_occupation_codes[14] = 'Transport-moving'\n",
    "dict_occupation_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_valid\n",
    "\n",
    "def add_new_edge(data, previous_edge_index, col_name): # col_name as a list? E.g. occupation AND hours per week\n",
    "    \n",
    "    if previous_edge_index is None:\n",
    "        previous_edges = np.array([], dtype=np.int32).reshape((0, 2))\n",
    "    \n",
    "    elif previous_edge_index is not None:\n",
    "        previous_edges = previous_edge_index.transpose()\n",
    "\n",
    "    # first, reset IDs\n",
    "    # to enable the computation of all combinations of clients sharing some attribute, i.e. column value (e.g. the same type of job)\n",
    "    data[\"clients_id\"] = data.reset_index().index\n",
    "    attribute_values = data[col_name].unique()\n",
    "    \n",
    "    for attribute in attribute_values:\n",
    "        # select clients with the same job\n",
    "        attribute_df = data[data[col_name] == attribute]\n",
    "        clients = attribute_df[\"clients_id\"].values        \n",
    "        # Build all combinations between clients with the same attribute e.g. job (without knowing their label)\n",
    "        permutations = list(itertools.combinations(clients, 2))\n",
    "        edges_source = [e[0] for e in permutations] # starting client -> to other client with the same attribute e.g. job\n",
    "        edges_target = [e[1] for e in permutations] # ending client -> from other client with the same attribute e.g. job\n",
    "        clients_edges = np.column_stack([edges_source, edges_target]) # convert combinations to array\n",
    "        # complete with each new attribute (e.g. new type of job), to get all couples of clients with the same attribute\n",
    "        previous_edges = np.vstack([previous_edges, clients_edges]) \n",
    "\n",
    "    # Convert to Pytorch Geometric format\n",
    "    edge_index = previous_edges.transpose()\n",
    "    # edge_index # [2, num_edges]\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    #edge_index = torch.from_numpy(edge_index)\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9192c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=X_valid\n",
    "previous_edge_index=None\n",
    "col_name='occupation'\n",
    "\n",
    "edge_occupation = add_new_edge(data, previous_edge_index, col_name)\n",
    "\n",
    "edge_occupation_capital_gain = add_new_edge(data, edge_occupation, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_names = [\"education\",\"sex\"] # since it seems relevant to combine the 2 features in a unique edge\n",
    "\n",
    "# first, reset IDs\n",
    "# to enable the computation of all combinations of clients sharing some attribute, i.e. column value (e.g. the same type of job)\n",
    "data[\"clients_id\"] = data.reset_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a76143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_edge(data, previous_edge_index, list_col_names): # with the list of columns to combine in the edge \n",
    "    \n",
    "    if previous_edge_index is None:\n",
    "        previous_edges = np.array([], dtype=np.int32).reshape((0, 2))\n",
    "    \n",
    "    elif previous_edge_index is not None:\n",
    "        previous_edges = previous_edge_index.transpose()\n",
    "\n",
    "    # first, reset IDs\n",
    "    # to enable the computation of all combinations of clients sharing some attribute, i.e. column value (e.g. the same type of job)\n",
    "    data[\"clients_id\"] = data.reset_index().index\n",
    "\n",
    "\n",
    "    if len(list_col_names)==1: # when a unique feature is chosen to form an edge\n",
    "        \n",
    "        col_name = list_col_names[0]\n",
    "        attribute_values = data[col_name].unique()\n",
    "\n",
    "        for attribute in attribute_values:\n",
    "            # select clients with the same job\n",
    "            attribute_df = data[data[col_name] == attribute]\n",
    "            clients = attribute_df[\"clients_id\"].values        \n",
    "            # Build all combinations between clients with the same attribute e.g. job (without knowing their label)\n",
    "            permutations = list(itertools.combinations(clients, 2))\n",
    "            edges_source = [e[0] for e in permutations] # starting client -> to other client with the same attribute e.g. job\n",
    "            edges_target = [e[1] for e in permutations] # ending client -> from other client with the same attribute e.g. job\n",
    "            clients_edges = np.column_stack([edges_source, edges_target]) # convert combinations to array\n",
    "            # complete with each new attribute (e.g. new type of job), to get all couples of clients with the same attribute\n",
    "            previous_edges = np.vstack([previous_edges, clients_edges]) \n",
    "    \n",
    "    elif len(list_col_names) == 2: # for the moment, maximum combination of 2 columns to create an edge\n",
    "    \n",
    "    # TODO join if too many categories (e.g. hours of work per week)\n",
    "    # else, 1050 combinations of types of jobs and hours per week - a bit hard to compute\n",
    "    # and irrelevant (mini-categories of clients as edges)...\n",
    "        col_1 = list_col_names[0]\n",
    "        col_2 = list_col_names[1]\n",
    "            \n",
    "        combinations_vals_cols_1_to_2 = np.array(np.meshgrid(data[col_1].unique(), data[col_2].unique())).T.reshape(-1,2)\n",
    "\n",
    "        for attr1, attr2 in combinations_vals_cols_1_to_2:\n",
    "            attribute_df = data.loc[(data[col_1] == attr1) & (data[col_2] == attr2)]\n",
    "            clients = attribute_df[\"clients_id\"].values        \n",
    "            # Build all combinations between clients with the same attribute e.g. job (without knowing their label)\n",
    "            permutations = list(itertools.combinations(clients, 2))\n",
    "            edges_source = [e[0] for e in permutations] # starting client -> to other client with the same attribute e.g. job\n",
    "            edges_target = [e[1] for e in permutations] # ending client -> from other client with the same attribute e.g. job\n",
    "            clients_edges = np.column_stack([edges_source, edges_target]) # convert combinations to array\n",
    "            # complete with each new attribute (e.g. new type of job), to get all couples of clients with the same attribute\n",
    "            previous_edges = np.vstack([previous_edges, clients_edges]) \n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"The maximum number of features you specify in list_col_names to create an edge must be 2.\")\n",
    "\n",
    "    # Convert to Pytorch Geometric format\n",
    "    edge_index = previous_edges.transpose()\n",
    "    # edge_index # [2, num_edges]\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    #edge_index = torch.from_numpy(edge_index)\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_valid\n",
    "previous_edge_index = None\n",
    "list_col_names = [\"sex\", \"education\"]\n",
    "\n",
    "edge_sex_education = add_new_edge(data=data, previous_edge_index=previous_edge_index, list_col_names=list_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18616ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_valid\n",
    "previous_edge_index = edge_sex_education\n",
    "list_col_names = [\"occupation\"]\n",
    "\n",
    "edge_sex_education_and_occupation = add_new_edge(data=data, previous_edge_index=previous_edge_index, list_col_names=list_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324bba9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO enhance the function (and then include it in the package)\n",
    "\n",
    "def table_to_graph(X, Y, list_col_names):\n",
    "    \n",
    "    #Make sure that we have no duplicate nodes\n",
    "    assert(X.index.unique().shape[0] == X.shape[0])\n",
    "    \n",
    "    # first of all, reset the IDs of clients\n",
    "    X[\"clients_id\"] = X.reset_index().index\n",
    "    \n",
    "    # Extract the node features\n",
    "\n",
    "        # The node features are typically represented in a matrix of the shape (num_nodes, node_feature_dim).\n",
    "        # For each of the bank clients, we simply extract their attributes (except here the \"occupation\", that would be used as an \"actionable\" edge to connect them)\n",
    "    list_X_cols = X.columns.to_list()\n",
    "    list_nodes_names = [col for col in list_X_cols if col not in list_col_names]\n",
    "    node_features = X[list_nodes_names]\n",
    "        # That's already our node feature matrix. The number of nodes and the ordering is implicitly defined by it's shape. Each row corresponds to one node in our final graph. \n",
    "    \n",
    "    # Convert to numpy\n",
    "    x = node_features.to_numpy()\n",
    "    # x.shape # [num_nodes x num_features]\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    x = torch.from_numpy(x)\n",
    "    \n",
    "    # Extract the labels\n",
    "    labels = Y\n",
    "        # Those are simply the wealthiness of each of the clients (if their income is >$50 000). This corresponds to a node-level prediction problem. \n",
    "        # Therefore we have as many labels as we have nodes.\n",
    "    \n",
    "    # to make the graph functioning, check that the nodes follow the same order than the labels (rows n°)\n",
    "        # else, sort values by ids\n",
    "    nb_corresponding_nodes_labels = (labels.index == node_features.index).sum()\n",
    "    assert(nb_corresponding_nodes_labels == X.shape[0])\n",
    "    \n",
    "    # Convert to numpy\n",
    "    y = labels.to_numpy()\n",
    "    #y.shape # [num_nodes, 1] --> node regression\n",
    "    # get the number of classes\n",
    "    num_classes=np.unique(y).shape[0]\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # Extract the edges, know with our function to combine columns \n",
    "    edges = add_new_edge(data=data, previous_edge_index=previous_edge_index, list_col_names=list_col_names)\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    edge_index = torch.from_numpy(edges)\n",
    "    \n",
    "    # finally, build the graph (if other attributes e.g. edge_features, you can also pass it there)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, num_classes=num_classes)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0102966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO enhance the function (and then include it in the package)\n",
    "\n",
    "def table_to_graph(X, Y, edge):\n",
    "    \n",
    "    #Make sure that we have no duplicate nodes\n",
    "    assert(X.index.unique().shape[0] == X.shape[0])\n",
    "    \n",
    "    # first of all, reset the IDs of clients\n",
    "    X[\"clients_id\"] = X.reset_index().index\n",
    "    \n",
    "    # Extract the node features\n",
    "\n",
    "        # The node features are typically represented in a matrix of the shape (num_nodes, node_feature_dim).\n",
    "        # For each of the bank clients, we simply extract their attributes (except here the \"occupation\", that would be used as an \"actionable\" edge to connect them)\n",
    "    node_features = X.loc[:, X.columns != edge]\n",
    "        # That's already our node feature matrix. The number of nodes and the ordering is implicitly defined by it's shape. Each row corresponds to one node in our final graph. \n",
    "    \n",
    "    # Convert to numpy\n",
    "    x = node_features.to_numpy()\n",
    "    # x.shape # [num_nodes x num_features]\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    x = torch.from_numpy(x)\n",
    "    \n",
    "    # Extract the labels\n",
    "    labels = Y\n",
    "        # Those are simply the wealthiness of each of the clients (if their income is >$50 000). This corresponds to a node-level prediction problem. \n",
    "        # Therefore we have as many labels as we have nodes.\n",
    "    \n",
    "    # to make the graph functioning, check that the nodes follow the same order than the labels (rows n°)\n",
    "        # else, sort values by ids\n",
    "    nb_corresponding_nodes_labels = (labels.index == node_features.index).sum()\n",
    "    assert(nb_corresponding_nodes_labels == X.shape[0])\n",
    "    \n",
    "    # Convert to numpy\n",
    "    y = labels.to_numpy()\n",
    "    #y.shape # [num_nodes, 1] --> node regression\n",
    "    # get the number of classes\n",
    "    num_classes=np.unique(y).shape[0]\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # Extract the edges\n",
    "        # That's probably the trickiest part with a tabular dataset. You need to think of a reasonable way to connect your nodes. \n",
    "        # We will use the type of job assignment here\n",
    "        # We now need to build all permutations of these clients within one type of job, which corresponds to a fully-connected graph within each occupation-subgroup. We use the column int_player_id as indices for the edges. If there is for example a [0, 1] in the edge index, it means that the first and second node (regarding the previously defined node feature matrix) are connected.\n",
    "    \n",
    "    jobs = X[\"occupation\"].unique()\n",
    "    all_edges = np.array([], dtype=np.int32).reshape((0, 2))\n",
    "    for job in jobs:\n",
    "        job_df = X[X[\"occupation\"] == job]\n",
    "        clients = job_df[\"clients_id\"].values        # Build all combinations, as all players are connected\n",
    "        permutations = list(itertools.combinations(clients, 2))\n",
    "        edges_source = [e[0] for e in permutations]\n",
    "        edges_target = [e[1] for e in permutations]\n",
    "        clients_edges = np.column_stack([edges_source, edges_target])\n",
    "        all_edges = np.vstack([all_edges, clients_edges])\n",
    "        \n",
    "    # begin with empty edge_index, to assess if the GNN structure works\n",
    "    #edge_index = torch.empty(2, 0, dtype=torch.long)\n",
    "        \n",
    "    # Convert to Pytorch Geometric format\n",
    "    edge_index = all_edges.transpose()\n",
    "    # edge_index # [2, num_edges]\n",
    "    # then convert to torch, for further compatibility avec the torch GNN\n",
    "    edge_index = torch.from_numpy(edge_index)\n",
    "    \n",
    "    # finally, build the graph (if other attributes e.g. edge_features, you can also pass it there)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y, num_classes=num_classes)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = table_to_graph(X=X_train, Y=Y_train, list_col_names=list_col_names)\n",
    "data_valid = table_to_graph(X=X_valid, Y=Y_valid, list_col_names=list_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bfed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54040b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classif_basic.graph import table_to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0063eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = table_to_graph(X=X_train, Y=Y_train, list_col_names=list_col_names)\n",
    "data_valid = table_to_graph(X=X_valid, Y=Y_valid, list_col_names=list_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b5cd3",
   "metadata": {},
   "source": [
    "# Train a basic Graph Neural Network on the graph-shaped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32385c",
   "metadata": {},
   "source": [
    "## Build a basic convolutional GNN with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ede7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here intervenes the quick \"introduction by example\" of GCN by torch\n",
    "# in 'https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f6f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_nb = 200\n",
    "\n",
    "t_basic_1 = time.time()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(data=data_train).to(device)\n",
    "data_train = data_train.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.double()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(batch_nb): \n",
    "    # better with 200 batches (with only feature \"occupation\" as edge, 70% accuracy vs 50% accuracy with 50 batches)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data_train)\n",
    "    loss = F.nll_loss(out, data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "t_basic_2 = time.time()\n",
    "\n",
    "print(f\"Training of the basic GCN on Census with {batch_nb} batches took {(t_basic_2 - t_basic_1)/60} mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d174f7",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the validation nodes. Obviously, linking the clients only through the job provides less than 70% of accuracy even on the train set. Therefore, we need to seek for other ways..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57509b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model(data_train).argmax(dim=1)\n",
    "nb_indivs_train = data_train.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_train = (pred_train == data_train.y).sum()\n",
    "acc = int(correct_train) / nb_indivs_train\n",
    "print(f'Accuracy on train data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7693ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = model(data_valid).argmax(dim=1)\n",
    "nb_indivs_valid = data_valid.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_valid = (pred_valid == data_valid.y).sum()\n",
    "acc = int(correct_valid) / nb_indivs_valid\n",
    "print(f'Accuracy on test data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c05d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connections of the node 1 with the other (14450-1) nodes\n",
    "# obviously there is a problem, as the node 18460 does not exist...\n",
    "data_train.edge_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442bd73",
   "metadata": {},
   "source": [
    "## Build a more complex GNN with torch\n",
    "The advantage of using torch_geometric to build the GNN is the compatibility with the graph of data, as data was just reshaped using torch_geometric (above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "num_data_classes = 2\n",
    "\n",
    "gcn_seq = Sequential('x, edge_index, batch', [\n",
    "    (Dropout(p=0.5), 'x -> x'),\n",
    "    (GCNConv(data_train.num_features, 64), 'x, edge_index -> x1'),\n",
    "    ReLU(inplace=True),\n",
    "    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n",
    "    ReLU(inplace=True),\n",
    "    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n",
    "    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n",
    "    (global_mean_pool, 'x, batch -> x'),\n",
    "    Linear(2 * 64, num_data_classes),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_gcn_seq(data, batch_nb):\n",
    "\n",
    "    t_seq_1 = time.time()\n",
    "    \n",
    "    if batch_nb is None:\n",
    "        batch_nb = 200\n",
    "\n",
    "    x = data.x.float()#.long()\n",
    "    edge_index = data.edge_index\n",
    "    batch = batch_nb*torch.ones(data.num_nodes).long() # set 200 batches with the required shape\n",
    "\n",
    "    pred = gcn_seq(x, edge_index, batch)\n",
    "\n",
    "    t_seq_2 = time.time()\n",
    "\n",
    "    print(f\"Predictions with the sequential GCN on Census with {batch_nb} batches took {round(t_seq_2 - t_seq_1)/60} mn\")\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698248d7",
   "metadata": {},
   "source": [
    "Obviously, using a more 'complex' model with the sole edge 'occupation' does not lead to better results (accuracy = 0.52, but without adapted features for training)... Then, we will try to constitute better edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d85766",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pred_gcn_seq(data=data_train, batch_nb=batch_nb)\n",
    "nb_indivs_train = data_train.x.shape[0]\n",
    "\n",
    "acc = int(correct_train) / nb_indivs_train\n",
    "print(f'Accuracy on train data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8078089",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pred_gcn_seq(data=data_valid, batch_nb=batch_nb)\n",
    "nb_indivs_valid = data_valid.x.shape[0]\n",
    "\n",
    "acc = int(correct_valid) / nb_indivs_valid\n",
    "print(f'Accuracy on valid data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00019023",
   "metadata": {},
   "source": [
    "Below were the previous tries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b198c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_train.x\n",
    "edge_index = data_train.edge_index\n",
    "batch = 10\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703ac06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "x = data_train.x\n",
    "edge_index = data_train.edge_index\n",
    "batch = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x = data_train.x\n",
    "    edge_index = data_train.edge_index\n",
    "    batch = 10\n",
    "    \n",
    "    out = model(x, edge_index, batch)\n",
    "    loss = F.nll_loss(data_train, data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e1278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = data_valid.x\n",
    "edge_index = data_valid.edge_index\n",
    "batch = 10\n",
    "\n",
    "pred_valid = model().argmax(dim=1)\n",
    "nb_indivs_valid = data_valid.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = (pred_valid == data_valid).sum()\n",
    "acc = int(correct) / nb_indivs_valid\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0328acf0",
   "metadata": {},
   "source": [
    "## Training a Graph Neural Network (GNN)\n",
    "\n",
    "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n",
    "\n",
    "Following-up on [the first part of the Torch tutorial we used](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8), we replace the linear layers by the [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) module.\n",
    "To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
    "In contrast, a single `Linear` layer is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
    "$$\n",
    "\n",
    "which does not make use of neighboring node information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d30e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_embedding(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    h = h.detach().cpu().numpy()\n",
    "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "    if epoch is not None and loss is not None:\n",
    "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb985e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data_train  # Get the first graph object.\n",
    "\n",
    "print(data)\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "#print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "#print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "#print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "# print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38b7c9",
   "metadata": {},
   "source": [
    "By printing edge_index, we can understand how PyG represents graph connectivity internally. We can see that for each edge, edge_index holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "edge_index = data.edge_index\n",
    "print(edge_index.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e916c",
   "metadata": {},
   "source": [
    "We can further visualize the graph by converting it to the networkx library format, which implements, in addition to graph manipulation functionalities, powerful tools for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67674d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "G = to_networkx(tf.convert_to_tensor(data), to_undirected=True)\n",
    "visualize_graph(G, color=tf.convert_to_tensor(data.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7fbba",
   "metadata": {},
   "source": [
    "Here, there was the code of yesterday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(data_train.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2) # number of classes on the data\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f0f05",
   "metadata": {},
   "source": [
    "**Embedding the Census Network**\n",
    "\n",
    "Let's take a look at the node embeddings produced by our GNN.\n",
    "Here, we pass in the initial node features `x` and the graph connectivity information `edge_index` to the model, and visualize its 2-dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, h = model(data_train.x, data_train.edge_index)\n",
    "print(f'Embedding shape: {list(h.shape)}')\n",
    "\n",
    "visualize_embedding(h, color=data_train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5f944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40841772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data_train.x, data_train.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, data_train.y)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data_valid.x, data_valid.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred == data_test.y  # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum()) / int(data_test.sum())  # Derive ratio of correct predictions.\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32837850",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_valid_set = augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e3a0f",
   "metadata": {},
   "source": [
    "We now see that this process with basic data preparation, modelling and integration of the results in a DataFrame (as storage of the model) is very fast (in seconds):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebefe615",
   "metadata": {},
   "source": [
    "features_importances_from_pickle(\n",
    "    augmented_train_valid_set=augmented_train_valid_set,\n",
    "    X_train_valid=X_train_valid,\n",
    "    model_task=model_task,\n",
    "    uncorrected_model_path=uncorrected_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "print(f\"Basic modelling took {round(t1 - t0)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c6a49",
   "metadata": {},
   "source": [
    "The further steps are for fairness assessment and correction of the model, functionality which is available with the package FairDream of DreamQuark (private for the moment)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48be396",
   "metadata": {},
   "source": [
    "## Detection alert (on train&valid data to examine if the model learned discriminant behavior)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4108de8c",
   "metadata": {},
   "source": [
    "augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddfaf680",
   "metadata": {},
   "source": [
    "train_valid_set_with_uncorrected_results = augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1da81ac9",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "augmented_train_valid_set = train_valid_set_with_uncorrected_results\n",
    "model_name = \"uncorrected\"\n",
    "\n",
    "fairness_purpose='percentage_positive'\n",
    "injustice_acceptance=1\n",
    "min_individuals_discrimined=0.01\n",
    "\n",
    "discrimination_alert(augmented_train_valid_set, model_name, fairness_purpose, model_task, injustice_acceptance, min_individuals_discrimined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6fb59",
   "metadata": {},
   "source": [
    "## Discrimination correction with a new fair model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbadc17",
   "metadata": {},
   "source": [
    "### Generating fairer models with grid search or weights distorsion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5e1dad5",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# the user determines one's fairness objectives to build new fairer models\n",
    "# on which group and regarding which criteria (purpose, constraint of the models) one aims to erase discrimination\n",
    "\n",
    "protected_attribute = 'education-num'\n",
    "\n",
    "# then the user sets the desired balance between stat and fair performances \n",
    "tradeoff = \"moderate\"\n",
    "weight_method = 'grid_and_weighted_groups'\n",
    "nb_fair_models = 6\n",
    "\n",
    "\n",
    "train_valid_set_with_corrected_results, models_df, best_model_dict = fair_train(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    train_valid_set_with_uncorrected_results=train_valid_set_with_uncorrected_results,\n",
    "    protected_attribute=protected_attribute,\n",
    "    fairness_purpose=fairness_purpose,\n",
    "    model_task=model_task,\n",
    "    stat_criteria=stat_criteria,\n",
    "    tradeoff=tradeoff,\n",
    "    weight_method=weight_method,\n",
    "    nb_fair_models=nb_fair_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35c752",
   "metadata": {},
   "source": [
    "### Evaluating the best fair model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39cd5d2d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fair_model_results(train_valid_set_with_corrected_results, models_df, best_model_dict,protected_attribute,fairness_purpose, model_task)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3cfd4f",
   "metadata": {},
   "source": [
    "top_models = models_df.sort_values(by='tradeoff_score',ascending=False)\n",
    "top_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

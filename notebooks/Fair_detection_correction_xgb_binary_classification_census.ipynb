{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Discrimination detection and mitigation (on revenues classification dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03693",
   "metadata": {},
   "source": [
    "## Train a model regardless of fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from fairdream.data_preparation import *\n",
    "from fairdream.compute_scores import *\n",
    "from fairdream.detection import *\n",
    "from fairdream.correction import *\n",
    "from fairdream.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107aa5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your statistics purposes\n",
    "model_task = 'classification'\n",
    "stat_criteria = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124240e",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Fix precise % of population distribution (sex: Male, Female) and % of loan granted according to sex, to inspect the effects of FairDream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a741869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset on clients for binary classification\n",
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = X.copy()\n",
    "dataset['target'] = Y\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, \"treatment\" is saw as being 'Male' and not 'Female'\n",
    "\n",
    "df_response_if_treated=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==1)]\n",
    "df_no_response_if_treated=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==0)]\n",
    "df_response_if_control=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==1)]\n",
    "df_no_response_if_control=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==0)]\n",
    "\n",
    "print(df_response_if_treated.shape[0])\n",
    "print(df_no_response_if_treated.shape[0])\n",
    "print(df_response_if_control.shape[0])\n",
    "print(df_no_response_if_control.shape[0])\n",
    "\n",
    "\n",
    "# % of men selected by the initial data\n",
    "df_response_if_treated.shape[0]/(df_response_if_treated.shape[0]+df_no_response_if_treated.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of women selected by the initial data\n",
    "df_response_if_control.shape[0]/(df_response_if_control.shape[0]+df_no_response_if_control.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset = 20_000\n",
    "\n",
    "is_treated='Male',\n",
    "percentage_treated= 50\n",
    "is_response= 'target'\n",
    "percentage_response_if_treated=70\n",
    "percentage_response_if_control=10\n",
    "\n",
    "sexist_dataset = set_marketing_treatment_effect(df_response_if_treated=df_response_if_treated,\n",
    "    df_no_response_if_treated=df_no_response_if_treated,\n",
    "    df_response_if_control=df_response_if_control,\n",
    "    df_no_response_if_control=df_no_response_if_control,\n",
    "    len_dataset=len_dataset,\n",
    "    is_treated=is_treated,\n",
    "    percentage_treated=percentage_treated,\n",
    "    is_response=is_response,\n",
    "    percentage_response_if_treated=percentage_response_if_treated,\n",
    "    percentage_response_if_control=percentage_response_if_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sexist_dataset.loc[: , dataset.columns != 'target']\n",
    "Y = sexist_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d78af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb9644",
   "metadata": {},
   "source": [
    "### Bring your own model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a8be6",
   "metadata": {},
   "source": [
    "If you want to bring your own model, you have to set 3 features:\n",
    "\n",
    "1. uncorrected_model_path\n",
    "Save your model in uncorrected_model_path, for fairness analysis on relevant features\n",
    "Ex: uncorrected_model_path = \"/work/data/models/uncorrected_model.pkl\"\n",
    "\n",
    "2. X_train_valid, Y_train_valid\n",
    "pd.DataFrame with your inputs and targets on train&valid set, of shape(nb_individuals,)\n",
    "\n",
    "3. Y_pred_train_valid\n",
    "np.ndarray with the predicted label (i.e. class) or value, of shape(nb_individuals,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447f8c",
   "metadata": {},
   "source": [
    "### Automatically train a model statistically performant, regardless of fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e01c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test = train_valid_test_split(X,Y, model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e26fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438539df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the uncorrected model, to then sort its features by importances\n",
    "save_model=True\n",
    "uncorrected_model_path = \"/work/data/models/uncorrected_model.pkl\"\n",
    "\n",
    "Y_pred_train_valid = train_naive_xgb(X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test, model_task, stat_criteria, save_model=save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48be396",
   "metadata": {},
   "source": [
    "## Detection alert (on train&valid data to examine if the model learned discriminant behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46799a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb28082",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_set_with_uncorrected_results = augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb89f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "augmented_train_valid_set = train_valid_set_with_uncorrected_results\n",
    "model_name = \"uncorrected\"\n",
    "\n",
    "fairness_purpose='percentage_positive'\n",
    "injustice_acceptance=1\n",
    "min_individuals_discrimined=0.01\n",
    "\n",
    "discrimination_alert(augmented_train_valid_set, model_name, fairness_purpose, model_task, injustice_acceptance, min_individuals_discrimined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6fb59",
   "metadata": {},
   "source": [
    "## Discrimination correction with a new fair model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbadc17",
   "metadata": {},
   "source": [
    "### Generating fairer models with grid search or weights distorsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482480b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the user determines one's fairness objectives to build new fairer models\n",
    "# on which group and regarding which criteria (purpose, constraint of the models) one aims to erase discrimination\n",
    "\n",
    "protected_attribute = 'education-num'\n",
    "\n",
    "# then the user sets the desired balance between stat and fair performances \n",
    "tradeoff = \"moderate\"\n",
    "weight_method = 'grid_and_weighted_groups'\n",
    "nb_fair_models = 6\n",
    "\n",
    "\n",
    "train_valid_set_with_corrected_results, models_df, best_model_dict = fair_train(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    train_valid_set_with_uncorrected_results=train_valid_set_with_uncorrected_results,\n",
    "    protected_attribute=protected_attribute,\n",
    "    fairness_purpose=fairness_purpose,\n",
    "    model_task=model_task,\n",
    "    stat_criteria=stat_criteria,\n",
    "    tradeoff=tradeoff,\n",
    "    weight_method=weight_method,\n",
    "    nb_fair_models=nb_fair_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35c752",
   "metadata": {},
   "source": [
    "### Evaluating the best fair model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c133ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fair_model_results(train_valid_set_with_corrected_results, models_df, best_model_dict,protected_attribute,fairness_purpose, model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11def59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = models_df.sort_values(by='tradeoff_score',ascending=False)\n",
    "top_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2affb517",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "In this notebook, we inspect **in which way a tabular dataset as Census can be used by an AI based on graphs to estimate wealthiness of individuals**. \n",
    "\n",
    "Therefore, we proceed in 2 steps:\n",
    "\n",
    "**1. We prepare data to be handled by a model based on a graph**\n",
    "We transform them into a graph, that involves strong assumptions on the features involved in connections...\n",
    "\n",
    "**2. We train an AI based on graphs**\n",
    "Here, we begin with a Graphical Neural Network (GNN) based on a Multi-Layer Perceptron (MLP), requiring the library Torch.\n",
    "\n",
    "**3. We inspect if the graph-based AI indeed reflects common & expert knowledge on**\n",
    "In particular, regarding the non-sense of certain inferences that should absolutely be avoided (e.g. education may influence occupation, but not the reverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Data preparation for binary classification with graphs (Census)\n",
    "For this reshaping (and also interpretation, see below the choice of edges) of data tables to graphs, we used a basic Google [colab](https://colab.research.google.com/drive/1_eR7DXBF3V4EwH946dDPOxeclDBeKNMD?usp=sharing#scrollTo=WuggdIItffpv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03693",
   "metadata": {},
   "source": [
    "## General preparation - handle categorical features\n",
    "Here, we handle the categorical features through label-encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676925f",
   "metadata": {},
   "source": [
    "As we need to install torch-scatter and torch-sparse to enable torch_geometric (enabling our transformation of data in table, and the GNN), which seem not compatible with GPU on poetry, we use a [trick](https://stackoverflow.com/questions/74823704/error-building-wheel-for-torch-sparse-error-installing-pytorch-geometric) to install them on notebook with pip (to be cleaned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771274e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    TORCH = torch.__version__.split(\"+\")[0]\n",
    "    CUDA = \"cu\" + torch.version.cuda.replace(\".\",\"\")\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "#!pip install torch-geometric\n",
    "#import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from classif_basic.data_preparation import train_valid_test_split, set_target_if_feature, automatic_preprocessing\n",
    "from classif_basic.data_preparation import handle_cat_features\n",
    "\n",
    "from classif_basic.graph import table_to_graph, add_new_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124240e",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a741869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset on clients for binary classification\n",
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7180da",
   "metadata": {},
   "source": [
    "### Add pre-processing: split hours-per-week in 2 quantiles, to use it as an edge (combined with \"occupation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7347572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"hours-per-week\"].value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52305d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_hours = X[\"hours-per-week\"].median() # '1' if the client works over 40 hours per week\n",
    "\n",
    "X[\"hours-per-week\"] = (X[\"hours-per-week\"] == median_hours).astype(int)\n",
    "X[\"hours-per-week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447f8c",
   "metadata": {},
   "source": [
    "### Train-test-split, to prepare for 3 graphs representing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e01c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task = \"classification\"\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test = train_valid_test_split(\n",
    "    X=X,\n",
    "    Y=Y, \n",
    "    model_task=model_task,\n",
    "    preprocessing_cat_features=preprocessing_cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99abfa",
   "metadata": {},
   "source": [
    "## Reshape (by interpreting) data to a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756db59",
   "metadata": {},
   "source": [
    "From this dataset (where we introduced selectively a \"sexist\" effect against women), let's see how we could swith from the tabular data to a graph representation.\n",
    "\n",
    "The point is that our features X all seem to be attributes of the clients, though we should find a way of representing their interactions between clients \n",
    "\n",
    "X = {race, age, sex, final weight (depends on age, sex, hispanic origin, race), education, education number, marital status, relationship, occupation, hours per week, workclass, race, sex, capital gain, capital loss, native country} \n",
    "\n",
    "**Nodes** \n",
    "Bank clients (by ID)\n",
    "\n",
    "**Edges** \n",
    "Here, we should find one or several ways of connecting the clients\n",
    "\n",
    "Should be occupation → if changes of occupation (or similar client with new occupation), which impact on the revenue? // change of football team => impact on the football rate \n",
    "(pers) actionable => predict revenue when switches to a new job??\n",
    "→ may be: “hours per week” <=> inspect the change of revenue if switches to greater hours per week?\n",
    "\n",
    "**Node Features** \n",
    "Attributs of the nodes, i.e. characteristics of the clients (here, hard to separate from what \"connects\" them...) \n",
    "\n",
    "Race, age, sex, final weight (depends on age, sex, hispanic origin, race), education, education number, marital status, relationship, hours per week, workclass, race, sex, capital gain, capital loss, native country \n",
    "\n",
    "**Label (here at a node-level?)** \n",
    "Income (Y = income > $50 000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7283a89",
   "metadata": {},
   "source": [
    "We will no more need the train/valid/test split outside, as we will get the train/valid/test masks => create a function returning the 3 masks, as X_total and Y_total label encoded? "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff88aab",
   "metadata": {},
   "source": [
    "# compute edge by hands: create our own edge combination, to predict the income - with directed paths\n",
    "# first edge joins \"occupation\" -> \"hours-per-week\"\n",
    "# second edge joins \"sex\" -> \"education\"\n",
    "\n",
    "edges_train = add_new_edge(data=X_train, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "#edges_train = add_new_edge(data=X_train, previous_edge=edges_train, list_col_names=[\"sex\",\"education\"])\n",
    "\n",
    "edges_valid = add_new_edge(data=X_valid, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "#edges_valid = add_new_edge(data=X_valid, previous_edge=edges_valid, list_col_names=[\"sex\",\"education\"])\n",
    "\n",
    "edges_test = add_new_edge(data=X_test, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "#edges_test = add_new_edge(data=X_test, previous_edge=edges_test, list_col_names=[\"sex\",\"education\"])\n",
    "\n",
    "# specify the feature(s) used to connect the clients in couples, i.e. to build the edge of the data graph\n",
    "\n",
    "list_col_names = [\"occupation\", \"hours-per-week\"]#, \"sex\",\"education\"]\n",
    "\n",
    "data_train = table_to_graph(X=X_train, Y=Y_train, list_col_names=list_col_names, edges=edges_train)\n",
    "data_valid = table_to_graph(X=X_valid, Y=Y_valid, list_col_names=list_col_names, edges=edges_valid)\n",
    "data_test = table_to_graph(X=X_test, Y=Y_test, list_col_names=list_col_names, edges=edges_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dac8c00",
   "metadata": {},
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4da3fe99",
   "metadata": {},
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "941204b6",
   "metadata": {},
   "source": [
    "data_valid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8591ea1a",
   "metadata": {},
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b5cd3",
   "metadata": {},
   "source": [
    "# Train a basic Graph Neural Network on the graph-shaped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5fdf7",
   "metadata": {},
   "source": [
    "Here are tries to train the model with batches..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c0771",
   "metadata": {},
   "source": [
    "Beta version (DataLoader was not made for graphs), to delete once our current functions are confirmed to work..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "51707a5f",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "counts = np.bincount(data_train.y)\n",
    "labels_weights = 1. / counts\n",
    "weights = labels_weights[data_train.y]\n",
    "WeightedRandomSampler(weights, len(weights))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5c92523",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from classif_basic.graph import train_GNN\n",
    "\n",
    "classifier = train_GNN(\n",
    "    data_train=data_train,\n",
    "    data_valid=data_valid)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83dbc7e1",
   "metadata": {},
   "source": [
    "np.bincount(data_train.y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de6d6ab3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from torch.utils.data import DataLoader as GraphDataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "class_counts = np.bincount(data_train.y)\n",
    "labels_weights = 1. / class_counts\n",
    "train_weights = labels_weights[data_train.y]\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=train_weights, num_samples=len(train_weights), replacement=True)\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "loader = GraphDataLoader(dataset=data_train,\n",
    "                        batch_size=batch_size,\n",
    "                        sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6878e",
   "metadata": {},
   "source": [
    "Test of my idea: create graphs with different edges, here sex (graph 1) -> education (graph 2)?\n",
    "\n",
    "Or enforce causal hierarchy through the neighborhood definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf0c92",
   "metadata": {},
   "source": [
    "As it is in use in the creation of batches by neighbors with PyTorch Geometric, we recreate the data.train_mask (i.e. boolean tensor indicating if the individual is in X_train) to pass in \"input_nodes\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d38034",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "X_train_mask = X_train.copy()\n",
    "X_train_mask[\"train_mask\"] = 1\n",
    "dict_train_mask = X_train_mask['train_mask'].to_dict()\n",
    "\n",
    "X_copy['train_mask'] = X_copy.index.map(dict_train_mask)\n",
    "\n",
    "X_copy['train_mask'] = X_copy['train_mask'].fillna(0)\n",
    "X_copy['train_mask'] = X_copy['train_mask'].astype(bool)\n",
    "\n",
    "X_copy['train_mask'] # TODO create the same mask to get data_valid and data_test\n",
    "train_mask = torch.tensor(X_copy['train_mask'].values)\n",
    "\n",
    "# TODO create the same mask to get Y_train values on Y_total, and then apply a proper loss \n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute edge by hands: create our own edge combination, to predict the income - with directed paths\n",
    "# first edge joins \"occupation\" -> \"hours-per-week\"\n",
    "# second edge joins \"sex\" -> \"education\"\n",
    "X_total = handle_cat_features(X=X, preprocessing_cat_features=\"label_encoding\")\n",
    "Y_total = Y.copy()\n",
    "\n",
    "list_col_names=[\"occupation\", \"hours-per-week\"] # test the model with only 2 categories (> or < median of work hours)\n",
    "\n",
    "edges_total = add_new_edge(data=X_total, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "#edges_total = add_new_edge(data=X_total, previous_edge=edges_total, list_col_names=[\"sex\",\"education\"]\n",
    "\n",
    "# for training by specifying \"masks\" (i.e. boolean for nodes = individuals selected to train the GNN), \n",
    "# add a specification on train indexes \n",
    "data_total = table_to_graph(X=X_total, Y=Y_total, list_col_names=list_col_names, edges=edges_total,\n",
    "                           train_mask=train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "loader = NeighborLoader(\n",
    "    data_total,\n",
    "    # Sample 30 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[30] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=128,\n",
    "    input_nodes=data_total.train_mask,\n",
    ")\n",
    "\n",
    "sampled_data = next(iter(loader))\n",
    "print(sampled_data.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd15a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(loader):\n",
    "    print(f\"{i} : \\n {data} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34681bc",
   "metadata": {},
   "source": [
    "Here, we try using the batches constituted from neighborhoods to train the GNN, using our GPU (if accessed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ff1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data_total.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, data_total.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bedc41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_nb = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "t_basic_1 = time.time()\n",
    "\n",
    "# activate and signal the use of GPU for faster processing\n",
    "if torch.cuda.is_available():    \n",
    "    print(\"Using GPU!\")    \n",
    "    device = torch.device(\"cuda\")\n",
    "    # torch.set_default_tensor_type('torch.cuda.FloatTensor')   \n",
    "else:    \n",
    "    print(\"Using CPU!\")       \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# initialize the structure of the classifier, and prepare for GNN training (with GPU)\n",
    "classifier = GCN().to(device) # classifier = GCN(data_total).to(device)\n",
    "\n",
    "# classifier = GraphClassifier(v_in=71, e_in=6, v_g=30, e_g=6, v_out=200, mc_out=22, i_types=11).float().to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print('starting training')\n",
    "classifier.train()\n",
    "\n",
    "for epoch in range(epoch_nb):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    classifier.train()\n",
    "    for i, data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        x = data.x.to(device)\n",
    "        edge_index=data.edge_index.to(device)\n",
    "        target = data.y.to(device)\n",
    "        preds = classifier(x=x.float(), edge_index=edge_index)\n",
    "        err = loss(preds, target)\n",
    "        _, preds_temp = torch.max(preds.data, 1)\n",
    "        total += len(target)\n",
    "        correct += (preds_temp == target).sum().item()\n",
    "        epoch_loss += err.item()\n",
    "        err.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1} Loss = {epoch_loss/(i+1)} Train Accuracy = {correct / total}') \n",
    "\n",
    "t_basic_2 = time.time()            \n",
    "print(f\"Training of the basic GCN on Census with {batch_size} batches and {epoch_nb} epochs took {(t_basic_2 - t_basic_1)/60} mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_basic_2 = time.time()            \n",
    "print(f\"Training of the basic GCN on Census on {data_total.x.shape[0]} nodes and {data_total.edge_index.shape[1]} edges, \\n with {sampled_data.batch_size} batches and {epoch_nb} epochs took {(t_basic_2 - t_basic_1)/60} mn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32385c",
   "metadata": {},
   "source": [
    "## Build a basic convolutional GNN with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ede7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here intervenes the quick \"introduction by example\" of GCN by torch\n",
    "# in 'https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318c1e8",
   "metadata": {},
   "source": [
    "Here, we try to reduce the use of GPU-memory by training through batches (built with torch.utils.data.DataLoader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b1e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 50\n",
    "batch_size = 100\n",
    "\n",
    "t_basic_1 = time.time()\n",
    "\n",
    "# activate and signal the use of GPU for faster processing\n",
    "if torch.cuda.is_available():    \n",
    "    print(\"Using GPU!\")    \n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')   \n",
    "else:    \n",
    "    print(\"Using CPU!\")       \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = GCN(data=data_train).to(device)\n",
    "data_train = data_train.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.double()\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data_train,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           generator=torch.Generator(device='cuda'))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for data_train_batch in train_loader:   # gives batch data\n",
    "        print(f\"\\n step {step} \\n\")\n",
    "        # Compute prediction and loss\n",
    "        out = model(data_train_batch)\n",
    "        loss = F.nll_loss(out, data_train_batch.y)\n",
    "        loss.backward()\n",
    "        del data_train_batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "t_basic_2 = time.time()\n",
    "        \n",
    "print(f\"Training of the basic GCN on Census with {batch_size} batches and {EPOCH} epochs took {(t_basic_2 - t_basic_1)/60} mn\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "100f518b",
   "metadata": {},
   "source": [
    "# classic training (with 200 epochs, without batches)\n",
    "\n",
    "gnn_training_set = data_train\n",
    "EPOCH = 200\n",
    "# batch_size = 20 # no batch used here, because voluntarily small data -> TODO use shuffling instead?\n",
    "\n",
    "t_basic_1 = time.time()\n",
    "\n",
    "# activate and signal the use of GPU for faster processing\n",
    "if torch.cuda.is_available():    \n",
    "    print(\"Using GPU!\")    \n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')   \n",
    "else:    \n",
    "    print(\"Using CPU!\")       \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = GCN(data=gnn_training_set).to(device)\n",
    "gnn_training_set = gnn_training_set.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.double()\n",
    "\n",
    "# model.train()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=gnn_training_set,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           generator=torch.Generator(device='cuda'))\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    optimizer.zero_grad() # reset gradients to null for each epoch\n",
    "    # Compute prediction and loss\n",
    "    out = model(gnn_training_set)\n",
    "    loss = F.nll_loss(out, gnn_training_set.y)\n",
    "    loss.backward()\n",
    "    optimizer.step() # add gradients results for the next epoch\n",
    "\n",
    "t_basic_2 = time.time()\n",
    "        \n",
    "print(f\"\\n Training of the basic GCN on Census with {batch_size} batches and {EPOCH} epochs took {(t_basic_2 - t_basic_1)/60} mn\")\n",
    "\n",
    "pred = model(gnn_training_set).argmax(dim=1)\n",
    "nb_indivs = gnn_training_set.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = (pred == gnn_training_set.y).sum()\n",
    "acc = int(correct) / nb_indivs\n",
    "print(f'\\n Accuracy on data used for training the GNN: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d174f7",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the validation nodes. Obviously, linking the clients only through the job provides less than 70% of accuracy even on the train set. Therefore, we need to seek for other ways..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f2b23",
   "metadata": {},
   "source": [
    "By creating an edge only with the combination of sex and education, we observe an accuracy of 61% on train that does not fall down on valid (65%). Moreover, **when the graph is directed (sex -> education), the accuracy seems to increase** without falling down valid performance: + 11% on train (76%), +2% on valid (67%), and 70% on test.  \n",
    "\n",
    "Thanks to the training of the GCN with 200 batches, which however took 20 mn for 15_000 rows and 2 classes (and we shall admit, edge_index=[2, 10813909])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9794ede",
   "metadata": {},
   "source": [
    "**Other observations (tests of combinations of features as edges)**\n",
    "\n",
    "Having created our own edge index combining (sex&education) and (occupation), the training took 7 mn more (27 mn) but the performance did not improve (61% on train set...)\n",
    "\n",
    "Adding the combination (occupation -> hours-per-week) to (sex -> education) does not improve the performances, but it decreases it (60 +-2 % on train and valid). Maybe because (i) it complexifies too much the network (ii) the model or (iii) the model's hyperparameters (batch, layers...) is too simple to catch these relations (iii) the models? \n",
    "\n",
    "**Constitution of couples graph data - graph networks to be tested, with input intervention changes...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57509b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = model(data_train).double().argmax(dim=1)\n",
    "nb_indivs_train = data_train.x.shape[0]\n",
    "\n",
    "model.double()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_train = (pred_train == data_train.y).sum()\n",
    "acc = int(correct_train) / nb_indivs_train\n",
    "print(f'Accuracy on train data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7693ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_valid = data_valid.to(device) # set data_valid in a GPU-compatible format\n",
    "\n",
    "pred_valid = model(data_valid).argmax(dim=1)\n",
    "nb_indivs_valid = data_valid.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_valid = (pred_valid == data_valid.y).sum()\n",
    "acc = int(correct_valid) / nb_indivs_valid\n",
    "print(f'Accuracy on valid data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b56e7a",
   "metadata": {},
   "source": [
    "Let's inspect the model on test data, to assess if the stability of performance is not due to coincidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa49910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_test = data_test.to(device) # set data_test in a GPU-compatible format\n",
    "\n",
    "pred_test = model(data_test).argmax(dim=1)\n",
    "nb_indivs_test = data_test.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_test = (pred_test == data_test.y).sum()\n",
    "acc = int(correct_test) / nb_indivs_test\n",
    "print(f'Accuracy on test data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad55b8b",
   "metadata": {},
   "source": [
    "# Visual Representation of the Graph\n",
    "Here, we will seek for a visual representation of the (directed acyclic?) graph. The goal is to check if it corresponds to the users' intuition - at least regarding the \"non sense\" causal paths. \n",
    "\n",
    "Here, the edges have been built with the directed path **sex -> education** (recall that the link [potentially] exists, because we voluntarily biased the data to be \"sexist\" regarding the distribution of incomes). Hence, the non-sense we don't want to find is an impact of education on sex. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "29cff6e8",
   "metadata": {},
   "source": [
    "# pip install --force-reinstall -v \"scipy==1.8\"\n",
    "!pip install --upgrade scipy networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78030ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6124b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "network_valid = to_networkx(data=data_valid)\n",
    "\n",
    "# subax1 = plt.subplot(121)\n",
    "\n",
    "# graph \n",
    "nx.draw(network_valid, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace804be",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col_names = [\"occupation\", \"hours-per-week\"]#, \"sex\",\"education\"]\n",
    "\n",
    "data_job_valid = table_to_graph(X=X_valid, Y=Y_valid, list_col_names=list_col_names, edges=edges_valid)\n",
    "\n",
    "network_job_valid = to_networkx(data=data_job_valid)\n",
    "nx.draw(network_job_valid, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6fe68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a representation of the edge (\"sex -> education\") \n",
    "# with only 2 values of education and 20 individuals (min, max)\n",
    "\n",
    "X_valid.reset_index(drop=True, inplace=True)\n",
    "Y_valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_education_max = X_valid.loc[X_valid[\"education\"]==X_valid[\"education\"].max()].iloc[:10]\n",
    "#df_education_min = X_valid.loc[X_valid[\"education\"]==X_valid[\"education\"].min()].iloc[:10]\n",
    "\n",
    "X_education_extreme = df_education_max#.append(df_education_min).sort_index()\n",
    "Y_education_extreme = Y_valid.iloc[X_education_extreme.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, gain a representation with only 10 individuals \n",
    "\n",
    "t_graph_0 = time.time()\n",
    "\n",
    "list_col_names = [\"sex\", \"education\"]\n",
    "\n",
    "edges_sex_valid = add_new_edge(data=X_education_extreme, previous_edge=None, list_col_names=list_col_names)\n",
    "\n",
    "data_sex_valid = table_to_graph(X=X_education_extreme, Y=Y_education_extreme, list_col_names=list_col_names, \n",
    "                                edges=edges_sex_valid)\n",
    "\n",
    "network_job_valid = to_networkx(data=data_job_valid)\n",
    "nx.draw(network_job_valid, with_labels=True, font_weight='bold')\n",
    "\n",
    "t_graph_1 = time.time()\n",
    "\n",
    "print(f\"Plotting the graph with {data_sex_valid.x.shape[0]} individuals took {(t_graph_1 - t_graph_0)/60} mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, gain a representation with only 10 individuals (and only 'sex' as edge)\n",
    "\n",
    "t_graph_0 = time.time()\n",
    "\n",
    "list_col_names = [\"sex\"]\n",
    "\n",
    "edges_sex_valid = add_new_edge(data=X_education_extreme, previous_edge=None, list_col_names=list_col_names)\n",
    "\n",
    "data_sex_valid = table_to_graph(X=X_education_extreme, Y=Y_education_extreme, list_col_names=list_col_names, \n",
    "                                edges=edges_sex_valid)\n",
    "\n",
    "network_job_valid = to_networkx(data=data_job_valid)\n",
    "nx.draw(network_job_valid, with_labels=True, font_weight='bold')\n",
    "\n",
    "t_graph_1 = time.time()\n",
    "\n",
    "print(f\"Plotting the graph with {data_sex_valid.x.shape[0]} individuals took {(t_graph_1 - t_graph_0)/60} mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ad121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, gain a representation with only 10 individuals (and only 'sex' as edge)\n",
    "\n",
    "t_graph_0 = time.time()\n",
    "\n",
    "list_col_names = [\"sex\"]\n",
    "\n",
    "edges_sex_valid = add_new_edge(data=X_education_extreme, previous_edge=None, list_col_names=list_col_names)\n",
    "\n",
    "data_sex_valid = table_to_graph(X=X_education_extreme, Y=Y_education_extreme, list_col_names=list_col_names, \n",
    "                                edges=edges_sex_valid)\n",
    "\n",
    "network_job_valid = to_networkx(data=data_job_valid)\n",
    "nx.draw(network_job_valid)\n",
    "\n",
    "t_graph_1 = time.time()\n",
    "\n",
    "print(f\"Plotting the graph with {data_sex_valid.x.shape[0]} individuals took {(t_graph_1 - t_graph_0)/60} mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a13ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, gain a representation with only 10 individuals (and only 'sex' as edge)\n",
    "\n",
    "t_graph_0 = time.time()\n",
    "\n",
    "list_col_names = ['capital-gain', 'capital-loss',\n",
    "       'hours-per-week', 'workclass', 'education', 'marital-status',\n",
    "       'occupation', 'relationship', 'race', 'sex', 'native-country',\n",
    "       'clients_id'] # to take only the likely 'relevant' features 'age', 'fnlwgt', 'education-num' as node\n",
    "\n",
    "edges_sex_valid = add_new_edge(data=X_education_extreme, previous_edge=None, list_col_names=['sex'])\n",
    "\n",
    "data_sex_valid = table_to_graph(X=X_education_extreme, Y=Y_education_extreme, list_col_names=list_col_names, \n",
    "                                edges=edges_sex_valid)\n",
    "\n",
    "network_job_valid = to_networkx(data=data_job_valid)\n",
    "nx.draw(network_job_valid)\n",
    "\n",
    "t_graph_1 = time.time()\n",
    "\n",
    "print(f\"Plotting the graph with {data_sex_valid.x.shape[0]} individuals took {(t_graph_1 - t_graph_0)/60} mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, gain a representation with only 10 individuals (and only 'sex' as edge)\n",
    "\n",
    "t_graph_0 = time.time()\n",
    "\n",
    "list_col_names = ['age', 'fnlwgt', 'capital-gain', 'capital-loss',\n",
    "       'hours-per-week', 'workclass', 'education', 'marital-status',\n",
    "       'occupation', 'relationship', 'race', 'sex', 'native-country',\n",
    "       'clients_id'] # to take only the likely 'relevant' feature 'education-num' as node\n",
    "\n",
    "edges_sex_valid = add_new_edge(data=X_education_extreme, previous_edge=None, list_col_names=['sex'])\n",
    "\n",
    "data_sex_valid = table_to_graph(X=X_education_extreme, Y=Y_education_extreme, list_col_names=list_col_names, \n",
    "                                edges=edges_sex_valid)\n",
    "\n",
    "network_job_valid = to_networkx(data=data_job_valid)\n",
    "nx.draw(network_job_valid)\n",
    "\n",
    "t_graph_1 = time.time()\n",
    "\n",
    "print(f\"Plotting the graph with {data_sex_valid.x.shape[0]} individuals took {(t_graph_1 - t_graph_0)/60} mn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0939edc4",
   "metadata": {},
   "source": [
    "Obviously, we have no clear intuition of what these links do correspond with... By individual, path from the sex to the income? But there are more groups than individuals here selected (10)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5823877",
   "metadata": {},
   "source": [
    "## Constitute a graph - Try to connect the features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76781366",
   "metadata": {},
   "source": [
    "Here, we proceed in 2 steps (back and forth)\n",
    "\n",
    "1. **Detect the relations**\n",
    "We use the partial dependance plots to inspect the correlations (pers) sufficient? Input intervention changes?\n",
    "\n",
    "1. **Select the causal direction**\n",
    "Based on the user's experience and expertise (e.g. sex -> education, because the contrary would be logically and temporally impossible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550aa49",
   "metadata": {},
   "source": [
    "At a first sight, look at correlated features (!) may be some hidden correlations => experience is still required at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstitute the dataset to check the correlations\n",
    "\n",
    "data_train_valid = X_train_valid.copy()\n",
    "data_train_valid['target'] = Y_train_valid\n",
    "data_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(19, 15))\n",
    "plt.matshow(data_train_valid.corr(), fignum=f.number)\n",
    "plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c529c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)    \n",
    "g = sns.pairplot(X_train_valid.filter(items=['education-num','education']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(X_train_valid.filter(items=['sex','age']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bbe1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# detect the relations: show the changes in predictions for the combinations of 2 features\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "f_names = [('sex', 'education')]\n",
    "# Similar to previous PDP plot except we use tuple of features instead of single feature\n",
    "disp4 = PartialDependenceDisplay.from_estimator(model, X_valid, f_names, ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

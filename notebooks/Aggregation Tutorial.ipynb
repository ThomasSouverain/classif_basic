{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial5: Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will override the aggregation method of the GIN convolution module of Pytorch Geometric implementing the following methods:\n",
    "\n",
    "- Principal Neighborhood Aggregation (PNA)\n",
    "- Learning Aggregation Functions (LAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Passing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(MessagePassing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the <span style='color:Blue'>aggregate</span> method, or, if you are using a sparse adjacency matrix, in the <span style='color:Blue'>message_and_aggregate</span> method. Convolutional classes in PyG extend MessagePassing, we construct our custom convoutional class extending GINConv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter operation in <span style='color:Blue'>aggregate</span>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rusty1s/pytorch_scatter/master/docs/source/_figures/add.svg?sanitize=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter, Module, Sigmoid\n",
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AbstractLAFLayer(Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AbstractLAFLayer, self).__init__()\n",
    "        assert 'units' in kwargs or 'weights' in kwargs\n",
    "        if 'device' in kwargs.keys():\n",
    "            self.device = kwargs['device']\n",
    "        else:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.ngpus = torch.cuda.device_count()\n",
    "        \n",
    "        if 'kernel_initializer' in kwargs.keys():\n",
    "            assert kwargs['kernel_initializer'] in [\n",
    "                'random_normal',\n",
    "                'glorot_normal',\n",
    "                'he_normal',\n",
    "                'random_uniform',\n",
    "                'glorot_uniform',\n",
    "                'he_uniform']\n",
    "            self.kernel_initializer = kwargs['kernel_initializer']\n",
    "        else:\n",
    "            self.kernel_initializer = 'random_normal'\n",
    "\n",
    "        if 'weights' in kwargs.keys():\n",
    "            self.weights = Parameter(kwargs['weights'].to(self.device), \\\n",
    "                                     requires_grad=True)\n",
    "            self.units = self.weights.shape[1]\n",
    "        else:\n",
    "            self.units = kwargs['units']\n",
    "            params = torch.empty(12, self.units, device=self.device)\n",
    "            if self.kernel_initializer == 'random_normal':\n",
    "                torch.nn.init.normal_(params)\n",
    "            elif self.kernel_initializer == 'glorot_normal':\n",
    "                torch.nn.init.xavier_normal_(params)\n",
    "            elif self.kernel_initializer == 'he_normal':\n",
    "                torch.nn.init.kaiming_normal_(params)\n",
    "            elif self.kernel_initializer == 'random_uniform':\n",
    "                torch.nn.init.uniform_(params)\n",
    "            elif self.kernel_initializer == 'glorot_uniform':\n",
    "                torch.nn.init.xavier_uniform_(params)\n",
    "            elif self.kernel_initializer == 'he_uniform':\n",
    "                torch.nn.init.kaiming_uniform_(params)\n",
    "            self.weights = Parameter(params, \\\n",
    "                                     requires_grad=True)\n",
    "        e = torch.tensor([1,-1,1,-1], dtype=torch.float32, device=self.device)\n",
    "        self.e = Parameter(e, requires_grad=False)\n",
    "        num_idx = torch.tensor([1,1,0,0], dtype=torch.float32, device=self.device).\\\n",
    "                                view(1,1,-1,1)\n",
    "        self.num_idx = Parameter(num_idx, requires_grad=False)\n",
    "        den_idx = torch.tensor([0,0,1,1], dtype=torch.float32, device=self.device).\\\n",
    "                                view(1,1,-1,1)\n",
    "        self.den_idx = Parameter(den_idx, requires_grad=False)\n",
    "        \n",
    "\n",
    "class LAFLayer(AbstractLAFLayer):\n",
    "    def __init__(self, eps=1e-7, **kwargs):\n",
    "        super(LAFLayer, self).__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, data, index, dim=0, **kwargs):\n",
    "        eps = self.eps\n",
    "        sup = 1.0 - eps \n",
    "        e = self.e\n",
    "\n",
    "        x = torch.clamp(data, eps, sup)\n",
    "        x = torch.unsqueeze(x, -1)\n",
    "        e = e.view(1,1,-1)        \n",
    "\n",
    "        exps = (1. - e)/2. + x*e \n",
    "        exps = torch.unsqueeze(exps, -1)\n",
    "        exps = torch.pow(exps, torch.relu(self.weights[0:4]))\n",
    "\n",
    "        scatter = torch_scatter.scatter_add(exps, index.view(-1), dim=dim)\n",
    "        scatter = torch.clamp(scatter, eps)\n",
    "\n",
    "        sqrt = torch.pow(scatter, torch.relu(self.weights[4:8]))\n",
    "        alpha_beta = self.weights[8:12].view(1,1,4,-1)\n",
    "        terms = sqrt * alpha_beta\n",
    "\n",
    "        num = torch.sum(terms * self.num_idx, dim=2)\n",
    "        den = torch.sum(terms * self.den_idx, dim=2)\n",
    "        \n",
    "        multiplier = 2.0*torch.clamp(torch.sign(den), min=0.0) - 1.0\n",
    "\n",
    "        den = torch.where((den < eps) & (den > -eps), multiplier*eps, den)\n",
    "\n",
    "        res = num / den\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAF Aggregation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLAFConv(GINConv):\n",
    "    def __init__(self, nn, units=1, node_dim=32, **kwargs): # TODO change dim of node embeddings? But how? \n",
    "        super(GINLAFConv, self).__init__(nn, **kwargs)\n",
    "        self.laf = LAFLayer(units=units, kernel_initializer='random_uniform')\n",
    "        self.mlp = torch.nn.Linear(node_dim*units, node_dim)\n",
    "        self.dim = node_dim\n",
    "        self.units = units\n",
    "        #self.batch_size = len(self.batch) # here we control TODO requires self.batch to be initialized\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        x = torch.sigmoid(inputs)\n",
    "        x = self.laf(x, index)\n",
    "        x = x.view((-1, self.dim * self.units))\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    # we add the forward, to control for the batch size (vs 945/946 error of model prediction)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = (x, x) # propagate_type: (x: OptPairTensor)\n",
    "        # here we control TODO requires self.batch to be initialized\n",
    "        batch_size=len(batch)\n",
    "        batch_size = (batch_size, batch_size) # double it as x=(x,x) information is duplicated\n",
    "        out = self.propagate(edge_index, x=x, size=batch_size)\n",
    "        \n",
    "        x_r = x[1]\n",
    "        #print(f\"x_r.shape: {x_r.shape}\")\n",
    "        #print(f\"out.shape: {out.shape} \\n\")        \n",
    "        if x_r is not None:\n",
    "            #print(\"no_null\")\n",
    "            out = out + (1 + self.eps) * x_r\n",
    "\n",
    "        return self.nn(out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " # alternative to \"block\" wrong dimensions (in the previous function)\n",
    "        # null signal if not the good shape\n",
    "        if x_r.shape != out.shape:\n",
    "            #print(\"null signal\")\n",
    "            out = (1 + self.eps) * x_r\n",
    "        \n",
    "        elif x_r is not None:\n",
    "            #print(\"no_null\")\n",
    "            out = out + (1 + self.eps) * x_r\n",
    "\n",
    "        return self.nn(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LAFNet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features #TODO pass data as input\n",
    "        dim = 9\n",
    "        units = 5\n",
    "        \n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINLAFConv(nn1, units=units, node_dim=num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINLAFConv(nn2, units=units, node_dim=dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINLAFConv(nn3, units=units, node_dim=dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINLAFConv(nn4, units=units, node_dim=dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINLAFConv(nn5, units=units, node_dim=dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \n",
    "        #self.batch_size=len(batch) # control for the batch size in forward propagation\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index, batch)) # we add batch in the BN (hmm, des Chocapic...)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index, batch))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index, batch))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index, batch))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index, batch))\n",
    "        x = self.bn5(x)\n",
    "        #print(f\"x just after bn1, ..., 5: {x.shape[0]}\")\n",
    "        # here, we test without global pooling -> useless batch indicator?\n",
    "        #x = global_add_pool(x, batch, size=len(batch)) # control for the passed size of batches\n",
    "        #print(f\"x just after pool (with batch): {x.shape[0]}\")\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing, SAGEConv, GINConv, global_add_pool\n",
    "import torch_scatter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join('./', 'data', 'TU')\n",
    "dataset = TUDataset(path, name='MUTAG').shuffle()\n",
    "test_dataset = dataset[:len(dataset) // 10]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are a few tests of mine..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports and train/test split (to be put in part 2. of the notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    TORCH = torch.__version__.split(\"+\")[0]\n",
    "    CUDA = \"cu\" + torch.version.cuda.replace(\".\",\"\")\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "#!pip install torch-geometric\n",
    "#import torch_geometric\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from classif_basic.data_preparation import handle_cat_features\n",
    "\n",
    "from classif_basic.graph.data_to_graph import table_to_graph, add_new_edge\n",
    "from classif_basic.graph.train import train_GNN_ancestor\n",
    "\n",
    "# preparing the dataset on clients for binary classification\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1\n",
    "\n",
    "SEED = 7\n",
    "VALID_SIZE = 0.15\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "X = handle_cat_features(X=X, preprocessing_cat_features=preprocessing_cat_features)\n",
    "\n",
    "# first of all, unify features with \"redundant\" causal information\n",
    "from classif_basic.graph.utils import get_unified_col\n",
    "\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"education\",\"education-num\"], new_col_name = \"education\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"relationship\",\"marital-status\"], new_col_name = \"relationship\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"occupation\",\"workclass\"], new_col_name = \"job\")\n",
    "X = get_unified_col(X=X, list_cols_to_join = [\"capital-gain\",\"capital-loss\"], new_col_name = \"capital\")\n",
    "\n",
    "# select equal proportion of classes \"wealthy\" and \"not wealthy\", and generates the new dataset accordingly\n",
    "from classif_basic.graph.utils import get_balanced_df\n",
    "\n",
    "balanced_df = get_balanced_df(X=X, Y=Y)\n",
    "\n",
    "X_balanced = balanced_df.drop(\"target\", axis=1)\n",
    "Y_balanced = balanced_df[\"target\"]\n",
    "\n",
    "#X=X_balanced # here, we try with the whole dataset (assuming it is imbalanced, but counts almost 50 000 nodes)\n",
    "#Y=Y_balanced\n",
    "\n",
    "# then, normalize the df categories for better neural-network computation\n",
    "from classif_basic.graph.utils import normalize_df\n",
    "\n",
    "X=normalize_df(df=X, normalization='min_max')\n",
    "\n",
    "# Split valid set for early stopping & model selection\n",
    "# \"stratify=Y\" to keep the same proportion of target classes in train/valid (i.e. model) and test sets \n",
    "X_model, X_test, Y_model, Y_test = train_test_split(\n",
    "    X, Y, test_size=VALID_SIZE, random_state=SEED, stratify=Y\n",
    ")\n",
    "\n",
    "#from classif_basic.graph.data_to_graph import table_to_graph\n",
    "\n",
    "#data_with_batch = table_to_graph(X=X_model,\n",
    "                                #Y=Y_model,\n",
    "                                #list_edges_names=['education', 'relationship'],\n",
    "                                #nb_batches=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_model=X_model.iloc[:-1]\n",
    "X_model.shape\n",
    "\n",
    "Y_model=Y_model.iloc[:-1]\n",
    "Y_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch size equally splitting individuals \n",
    "# TODO check of user inputs (batch_size, nb_batches) before LAF computation\n",
    "\n",
    "def smallest_prime_factor(x):\n",
    "    \"\"\"Returns the smallest prime number that is a divisor of x\"\"\"\n",
    "    # Start checking with 2, then move up one by one\n",
    "    n = 2\n",
    "    while n*n <= x:\n",
    "        if x % n == 0:\n",
    "            return n\n",
    "        n += 1\n",
    "    return x\n",
    "\n",
    "pgcd_model = smallest_prime_factor(X_model.shape[0])\n",
    "pgcd_model = smallest_prime_factor(X_model.shape[0]/pgcd_model)\n",
    "\n",
    "pgcd_test = smallest_prime_factor(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_batches_model = X_model.shape[0]/pgcd_model\n",
    "print(nb_batches_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_batches_test = X_test.shape[0]/pgcd_test\n",
    "print(nb_batches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from classif_basic.model import pickle_load_model\n",
    "from classif_basic.graph.loader import get_loader\n",
    "\n",
    "#data_with_batch=pickle_load_model(\"/work/data/graph_data/unbalanced/data_full_features_education_relationship.pkl\")\n",
    "\n",
    "#data_with_batch_test=pickle_load_model(\"work/data/graph_data/unbalanced/test/data_full_features_education_relationship.pkl\")\n",
    "\n",
    "# TODO explicit: pass sizes to LAF module\n",
    "data_with_batch = table_to_graph(X=X_model,\n",
    "                                Y=Y_model,\n",
    "                                list_edges_names=['education', 'relationship'],\n",
    "                                nb_batches=nb_batches_model) # TODO change name in batch_size?\n",
    "\n",
    "data_with_batch_test = table_to_graph(X=X_test,\n",
    "                                Y=Y_test,\n",
    "                                list_edges_names=['education', 'relationship'],\n",
    "                                nb_batches=nb_batches_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_method='index_groups'\n",
    "\n",
    "print(\"Train&Valid Set\")\n",
    "train_loader = get_loader(data_total=data_with_batch, \n",
    "                    loader_method=loader_method,\n",
    "                    batch_size=nb_batches_model)\n",
    "\n",
    "print(\"Test Set\")\n",
    "test_loader = get_loader(data_total=data_with_batch_test, \n",
    "                    loader_method=loader_method,\n",
    "                    batch_size=nb_batches_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "dataset=data_with_batch # TODO explicit: pass it as class LAFNet input\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = \"LAF\"\n",
    "if net == \"LAF\":\n",
    "    model = LAFNet().to(device)\n",
    "elif net == \"PNA\":\n",
    "    model = PNANet().to(device)\n",
    "elif net == \"GIN\":\n",
    "    GINNet().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    list_batches_loss = []\n",
    "    # test with the 6 first batches (which hold an appropriate shape)\n",
    "    for i, data in enumerate(train_loader): #zip(range(7), train_loader):\n",
    "        #print(f\"Training of subgraph {i}\")\n",
    "        data.x = data.x.to(torch.float32) # to avoid dtypes differences\n",
    "        #data.edge_index = data.edge_index.to(torch.float32) # to avoid dtypes differences\n",
    "        #data.batch = data.batch.to(torch.float32) # to avoid dtypes differences\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        #print(f\"len(data.x): {len(data.x)}\")\n",
    "        #print(f\"len(output): {len(output)}\")\n",
    "        #print(f\"len(data.y): {len(data.y)}\")\n",
    "        class_weights=class_weight.compute_class_weight(class_weight='balanced', # alternative: dict_class_weights for large imbalance?\n",
    "                                                        classes=np.unique(data.y.cpu()),\n",
    "                                                        y=data.y.cpu().numpy())\n",
    "        class_weights=torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "        cr_loss = torch.nn.CrossEntropyLoss(weight=class_weights,reduction='mean')  \n",
    "        loss=cr_loss(output, data.y)\n",
    "        #loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        list_batches_loss.append(loss.item())\n",
    "        #loss_all += loss.item() #* data.num_graphs\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_mean = round(statistics.mean(list_batches_loss), 2)\n",
    "    \n",
    "    return loss_mean\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    accuracy_total=0\n",
    "    list_batches_accuracy = []\n",
    "    \n",
    "    for i, data in enumerate(loader):\n",
    "    #for i, data in zip(range(7),loader):\n",
    "        data.x = data.x.to(torch.float32) # to avoid dtypes differences\n",
    "        #data.edge_index = data.edge_index.to(torch.float32) # to avoid dtypes differences\n",
    "        #data.batch = data.batch.to(torch.float32) # to avoid dtypes differences\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct = pred.eq(data.y).sum().item()\n",
    "        total = len(data.y)\n",
    "        accuracy_sample = round(correct/total, 2)\n",
    "        list_batches_accuracy.append(accuracy_sample)\n",
    "        #print(f\"Accuracy on training batch {i}: {accuracy_sample}\")\n",
    "        \n",
    "    accuracy_mean = round(statistics.mean(list_batches_accuracy), 2)\n",
    "    \n",
    "    return accuracy_mean\n",
    "\n",
    "time0=time.time()\n",
    "\n",
    "epoch_nb = 1\n",
    "for epoch in range(1, 15+1):\n",
    "    print(f\"\\n '''Epoch {epoch_nb}''' \")\n",
    "    train_loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
    "          'Train Acc: {:.7f}, Test Acc: {:.7f}'.format(epoch, train_loss,\n",
    "                                                       train_acc, test_acc))\n",
    "    print()\n",
    "    epoch_nb = epoch_nb + 1\n",
    "\n",
    "time1=time.time()\n",
    "\n",
    "print(f\"Training LAF took {round((time1-time0)/60)} mn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "152px",
    "width": "247px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2affb517",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "In this notebook, we inspect **in which way a tabular dataset as Census can be used by an AI based on graphs to estimate wealthiness of individuals**. \n",
    "\n",
    "Therefore, we proceed in 2 steps:\n",
    "\n",
    "**1. We prepare data to be handled by a model based on a graph**\n",
    "We transform them into a graph, that involves strong assumptions on the features involved in connections...\n",
    "\n",
    "**2. We train an AI based on graphs**\n",
    "Here, we begin with a Graphical Neural Network (GNN) based on a Multi-Layer Perceptron (MLP), requiring the library Torch.\n",
    "\n",
    "**3. We inspect if the graph-based AI indeed reflects common & expert knowledge on**\n",
    "In particular, regarding the non-sense of certain inferences that should absolutely be avoided (e.g. education may influence occupation, but not the reverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58dd94",
   "metadata": {},
   "source": [
    "# Data preparation for binary classification with graphs (Census)\n",
    "For this reshaping (and also interpretation, see below the choice of edges) of data tables to graphs, we based on https://colab.research.google.com/drive/1_eR7DXBF3V4EwH946dDPOxeclDBeKNMD?usp=sharing#scrollTo=WuggdIItffpv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03693",
   "metadata": {},
   "source": [
    "## General preparation - handle categorical features\n",
    "Here, we handle the categorical features through label-encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import time\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from classif_basic.data_preparation import train_valid_test_split, set_target_if_feature, automatic_preprocessing\n",
    "\n",
    "from classif_basic.graph import table_to_graph, add_new_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124240e",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Fix precise % of population distribution (sex: Male, Female) and % of wealthiness according to sex. In that way, we could inspect if the structure of the model (here based on a graph) integrates this \"sexist\" representation of the world. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a741869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset on clients for binary classification\n",
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "X = data.data\n",
    "Y = (data.target == '>50K') * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = X.copy()\n",
    "dataset['target'] = Y\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, \"treatment\" is saw as being 'Male' and not 'Female'\n",
    "\n",
    "df_response_if_feature=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==1)]\n",
    "df_no_response_if_feature=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==0)]\n",
    "df_response_if_not_feature=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==1)]\n",
    "df_no_response_if_not_feature=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==0)]\n",
    "\n",
    "print(df_response_if_feature.shape[0])\n",
    "print(df_no_response_if_feature.shape[0])\n",
    "print(df_response_if_not_feature.shape[0])\n",
    "print(df_no_response_if_not_feature.shape[0])\n",
    "\n",
    "\n",
    "# % of men selected by the initial data\n",
    "df_response_if_feature.shape[0]/(df_response_if_feature.shape[0]+df_no_response_if_feature.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of women selected by the initial data\n",
    "df_response_if_not_feature.shape[0]/(df_response_if_feature.shape[0]+df_no_response_if_not_feature.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset = 20_000\n",
    "\n",
    "percentage_feature= 70\n",
    "percentage_response_if_feature=70\n",
    "percentage_response_if_not_feature=10\n",
    "\n",
    "sexist_dataset = set_target_if_feature(\n",
    "    df_response_if_feature=df_response_if_feature,\n",
    "    df_no_response_if_feature=df_no_response_if_feature,\n",
    "    df_response_if_not_feature=df_response_if_not_feature,\n",
    "    df_no_response_if_not_feature=df_no_response_if_not_feature,\n",
    "    len_dataset=len_dataset,\n",
    "    percentage_feature=percentage_feature,\n",
    "    percentage_response_if_feature=percentage_response_if_feature,\n",
    "    percentage_response_if_not_feature=percentage_response_if_not_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sexist_dataset.loc[: , dataset.columns != 'target']\n",
    "Y = sexist_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d78af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7180da",
   "metadata": {},
   "source": [
    "### Add pre-processing: split hours-per-week in 2 quantiles, to use it as an edge (combined with \"occupation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7347572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"hours-per-week\"].value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52305d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_hours = X[\"hours-per-week\"].median() # '1' if the client works over 40 hours per week\n",
    "\n",
    "X[\"hours-per-week\"] = (X[\"hours-per-week\"] == median_hours).astype(int)\n",
    "X[\"hours-per-week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447f8c",
   "metadata": {},
   "source": [
    "### Train-test-split, to prepare for 3 graphs representing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e01c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_task = \"classification\"\n",
    "preprocessing_cat_features = \"label_encoding\"\n",
    "\n",
    "X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test = train_valid_test_split(\n",
    "    X=X,\n",
    "    Y=Y, \n",
    "    model_task=model_task,\n",
    "    preprocessing_cat_features=preprocessing_cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99abfa",
   "metadata": {},
   "source": [
    "## Reshape (by interpreting) data to a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756db59",
   "metadata": {},
   "source": [
    "From this dataset (where we introduced selectively a \"sexist\" effect against women), let's see how we could swith from the tabular data to a graph representation.\n",
    "\n",
    "The point is that our features X all seem to be attributes of the clients, though we should find a way of representing their interactions between clients \n",
    "\n",
    "X = {race, age, sex, final weight (depends on age, sex, hispanic origin, race), education, education number, marital status, relationship, occupation, hours per week, workclass, race, sex, capital gain, capital loss, native country} \n",
    "\n",
    "**Nodes** \n",
    "Bank clients (by ID)\n",
    "\n",
    "**Edges** \n",
    "Here, we should find one or several ways of connecting the clients\n",
    "\n",
    "Should be occupation → if changes of occupation (or similar client with new occupation), which impact on the revenue? // change of football team => impact on the football rate \n",
    "(pers) actionable => predict revenue when switches to a new job??\n",
    "→ may be: “hours per week” <=> inspect the change of revenue if switches to greater hours per week?\n",
    "\n",
    "**Node Features** \n",
    "Attributs of the nodes, i.e. characteristics of the clients (here, hard to separate from what \"connects\" them...) \n",
    "\n",
    "Race, age, sex, final weight (depends on age, sex, hispanic origin, race), education, education number, marital status, relationship, hours per week, workclass, race, sex, capital gain, capital loss, native country \n",
    "\n",
    "**Label (here at a node-level?)** \n",
    "Income (Y = income > $50 000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute edge by hands: create our own edge combination, to predict the income - with directed paths\n",
    "# first edge joins \"occupation\" -> \"hours-per-week\"\n",
    "# second edge joins \"sex\" -> \"education\"\n",
    "\n",
    "edges_train = add_new_edge(data=X_train, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "edges_train = add_new_edge(data=X_train, previous_edge=edges_train, list_col_names=[\"sex\",\"education\"])\n",
    "\n",
    "edges_valid = add_new_edge(data=X_valid, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "edges_valid = add_new_edge(data=X_valid, previous_edge=edges_valid, list_col_names=[\"sex\",\"education\"])\n",
    "\n",
    "edges_test = add_new_edge(data=X_test, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "edges_test = add_new_edge(data=X_test, previous_edge=edges_test, list_col_names=[\"sex\",\"education\"])\n",
    "\n",
    "# specify the feature(s) used to connect the clients in couples, i.e. to build the edge of the data graph\n",
    "\n",
    "list_col_names = [\"occupation\", \"hours-per-week\", \"sex\",\"education\"]\n",
    "\n",
    "data_train = table_to_graph(X=X_train, Y=Y_train, list_col_names=list_col_names, edges=edges_train)\n",
    "data_valid = table_to_graph(X=X_valid, Y=Y_valid, list_col_names=list_col_names, edges=edges_valid)\n",
    "data_test = table_to_graph(X=X_test, Y=Y_test, list_col_names=list_col_names, edges=edges_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3587b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0063eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic edge with the features \"sex\" -> \"education\" (having added that the edges must be \"directed\")\n",
    "list_col_names = [\"sex\",\"education\"]\n",
    "\n",
    "data_train = table_to_graph(X=X_train, Y=Y_train, list_col_names=list_col_names)\n",
    "data_valid = table_to_graph(X=X_valid, Y=Y_valid, list_col_names=list_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b5cd3",
   "metadata": {},
   "source": [
    "# Train a basic Graph Neural Network on the graph-shaped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32385c",
   "metadata": {},
   "source": [
    "## Build a basic convolutional GNN with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ede7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here intervenes the quick \"introduction by example\" of GCN by torch\n",
    "# in 'https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f6f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_nb = 200\n",
    "\n",
    "t_basic_1 = time.time()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(data=data_train).to(device)\n",
    "data_train = data_train.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.double()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(batch_nb): \n",
    "    # better with 200 batches (with only feature \"occupation\" as edge, 70% accuracy vs 50% accuracy with 50 batches)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data_train)\n",
    "    loss = F.nll_loss(out, data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "t_basic_2 = time.time()\n",
    "\n",
    "print(f\"Training of the basic GCN on Census with {batch_nb} batches took {(t_basic_2 - t_basic_1)/60} mn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d174f7",
   "metadata": {},
   "source": [
    "Finally, we can evaluate our model on the validation nodes. Obviously, linking the clients only through the job provides less than 70% of accuracy even on the train set. Therefore, we need to seek for other ways..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f2b23",
   "metadata": {},
   "source": [
    "By creating an edge only with the combination of sex and education, we observe an accuracy of 61% on train that does not fall down on valid (65%). Moreover, **when the graph is directed (sex -> education), the accuracy seems to increase** without falling down valid performance: + 11% on train (76%), +2% on valid (67%), and 70% on test.  \n",
    "\n",
    "Thanks to the training of the GCN with 200 batches, which however took 20 mn for 15_000 rows and 2 classes (and we shall admit, edge_index=[2, 10813909])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9794ede",
   "metadata": {},
   "source": [
    "**Other observations (tests of combinations of features as edges)**\n",
    "\n",
    "Having created our own edge index combining (sex&education) and (occupation), the training took 7 mn more (27 mn) but the performance did not improve (61% on train set...)\n",
    "\n",
    "Adding the combination (occupation -> hours-per-week) to (sex -> education) does not improve the performances, but it decreases it (60 +-2 % on train and valid). Maybe because (i) it complexifies too much the network (ii) the model or (iii) the model's hyperparameters (batch, layers...) is too simple to catch these relations (iii) the models? \n",
    "\n",
    "**Constitution of couples graph data - graph networks to be tested, with input intervention changes...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57509b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model(data_train).argmax(dim=1)\n",
    "nb_indivs_train = data_train.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_train = (pred_train == data_train.y).sum()\n",
    "acc = int(correct_train) / nb_indivs_train\n",
    "print(f'Accuracy on train data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7693ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_valid = model(data_valid).argmax(dim=1)\n",
    "nb_indivs_valid = data_valid.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_valid = (pred_valid == data_valid.y).sum()\n",
    "acc = int(correct_valid) / nb_indivs_valid\n",
    "print(f'Accuracy on valid data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b56e7a",
   "metadata": {},
   "source": [
    "Let's inspect the model on test data, to assess if the stability of performance is not due to coincidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa49910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test = model(data_test).argmax(dim=1)\n",
    "nb_indivs_test = data_test.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_test = (pred_test == data_test.y).sum()\n",
    "acc = int(correct_test) / nb_indivs_test\n",
    "print(f'Accuracy on test data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad55b8b",
   "metadata": {},
   "source": [
    "# Visual Representation of the Graph\n",
    "Here, we will seek for a visual representation of the (directed acyclic?) graph. The goal is to check if it corresponds to the users' intuition - at least regarding the \"non sense\" causal paths. \n",
    "\n",
    "Here, the edges have been built with the directed path **sex -> education** (recall that the link [potentially] exists, because we voluntarily biased the data to be \"sexist\" regarding the distribution of incomes). Hence, the non-sense we don't want to find is an impact of education on sex. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "29cff6e8",
   "metadata": {},
   "source": [
    "# pip install --force-reinstall -v \"scipy==1.8\"\n",
    "!pip install --upgrade scipy networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78030ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f990211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute edge by hands: create our own edge combination, to predict the income - with directed paths\n",
    "# first edge joins \"occupation\" -> \"hours-per-week\"\n",
    "# second edge joins \"sex\" -> \"education\"\n",
    "#edges_valid = add_new_edge(data=X_valid, previous_edge=None, list_col_names=[\"occupation\", \"hours-per-week\"])\n",
    "edges_valid = add_new_edge(data=X_valid, previous_edge=None, list_col_names=[\"sex\",\"education\"])\n",
    "\n",
    "# specify the feature(s) used to connect the clients in couples, i.e. to build the edge of the data graph\n",
    "\n",
    "list_col_names = [\"occupation\", \"hours-per-week\", \"sex\",\"education\"]\n",
    "\n",
    "data_valid = table_to_graph(X=X_valid, Y=Y_valid, list_col_names=list_col_names, edges=edges_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6124b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_valid = to_networkx(data=data_valid)\n",
    "\n",
    "# subax1 = plt.subplot(121)\n",
    "\n",
    "nx.draw(network_valid, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442bd73",
   "metadata": {},
   "source": [
    "## Build a more complex GNN with torch\n",
    "The advantage of using torch_geometric to build the GNN is the compatibility with the graph of data, as data was just reshaped using torch_geometric (above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "num_data_classes = 2\n",
    "\n",
    "gcn_seq = Sequential('x, edge_index, batch', [\n",
    "    (Dropout(p=0.5), 'x -> x'),\n",
    "    (GCNConv(data_train.num_features, 64), 'x, edge_index -> x1'),\n",
    "    ReLU(inplace=True),\n",
    "    (GCNConv(64, 64), 'x1, edge_index -> x2'),\n",
    "    ReLU(inplace=True),\n",
    "    (lambda x1, x2: [x1, x2], 'x1, x2 -> xs'),\n",
    "    (JumpingKnowledge(\"cat\", 64, num_layers=2), 'xs -> x'),\n",
    "    (global_mean_pool, 'x, batch -> x'),\n",
    "    Linear(2 * 64, num_data_classes),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_gcn_seq(data, batch_nb):\n",
    "\n",
    "    t_seq_1 = time.time()\n",
    "    \n",
    "    if batch_nb is None:\n",
    "        batch_nb = 200\n",
    "\n",
    "    x = data.x.float()#.long()\n",
    "    edge_index = data.edge_index\n",
    "    batch = batch_nb*torch.ones(data.num_nodes).long() # set 200 batches with the required shape\n",
    "\n",
    "    pred = gcn_seq(x, edge_index, batch)\n",
    "\n",
    "    t_seq_2 = time.time()\n",
    "\n",
    "    print(f\"Predictions with the sequential GCN on Census with {batch_nb} batches took {round(t_seq_2 - t_seq_1)/60} mn\")\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698248d7",
   "metadata": {},
   "source": [
    "Obviously, using a more 'complex' model with the sole edge 'occupation' does not lead to better results (accuracy = 0.52, but without adapted features for training)... Then, we will try to constitute better edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d85766",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pred_gcn_seq(data=data_train, batch_nb=batch_nb)\n",
    "nb_indivs_train = data_train.x.shape[0]\n",
    "\n",
    "acc = int(correct_train) / nb_indivs_train\n",
    "print(f'Accuracy on train data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8078089",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pred_gcn_seq(data=data_valid, batch_nb=batch_nb)\n",
    "nb_indivs_valid = data_valid.x.shape[0]\n",
    "\n",
    "acc = int(correct_valid) / nb_indivs_valid\n",
    "print(f'Accuracy on valid data: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00019023",
   "metadata": {},
   "source": [
    "Below were the previous tries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b198c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_train.x\n",
    "edge_index = data_train.edge_index\n",
    "batch = 10\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703ac06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "x = data_train.x\n",
    "edge_index = data_train.edge_index\n",
    "batch = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x = data_train.x\n",
    "    edge_index = data_train.edge_index\n",
    "    batch = 10\n",
    "    \n",
    "    out = model(x, edge_index, batch)\n",
    "    loss = F.nll_loss(data_train, data_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e1278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = data_valid.x\n",
    "edge_index = data_valid.edge_index\n",
    "batch = 10\n",
    "\n",
    "pred_valid = model().argmax(dim=1)\n",
    "nb_indivs_valid = data_valid.x.shape[0]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = (pred_valid == data_valid).sum()\n",
    "acc = int(correct) / nb_indivs_valid\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0328acf0",
   "metadata": {},
   "source": [
    "## Training a Graph Neural Network (GNN)\n",
    "\n",
    "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n",
    "\n",
    "Following-up on [the first part of the Torch tutorial we used](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8), we replace the linear layers by the [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) module.\n",
    "To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
    "In contrast, a single `Linear` layer is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
    "$$\n",
    "\n",
    "which does not make use of neighboring node information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d30e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_embedding(h, color, epoch=None, loss=None):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    h = h.detach().cpu().numpy()\n",
    "    plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
    "    if epoch is not None and loss is not None:\n",
    "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb985e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data_train  # Get the first graph object.\n",
    "\n",
    "print(data)\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "#print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "#print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "#print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "# print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38b7c9",
   "metadata": {},
   "source": [
    "By printing edge_index, we can understand how PyG represents graph connectivity internally. We can see that for each edge, edge_index holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "edge_index = data.edge_index\n",
    "print(edge_index.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e916c",
   "metadata": {},
   "source": [
    "We can further visualize the graph by converting it to the networkx library format, which implements, in addition to graph manipulation functionalities, powerful tools for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67674d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "G = to_networkx(tf.convert_to_tensor(data), to_undirected=True)\n",
    "visualize_graph(G, color=tf.convert_to_tensor(data.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7fbba",
   "metadata": {},
   "source": [
    "Here, there was the code of yesterday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(data_train.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2) # number of classes on the data\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f0f05",
   "metadata": {},
   "source": [
    "**Embedding the Census Network**\n",
    "\n",
    "Let's take a look at the node embeddings produced by our GNN.\n",
    "Here, we pass in the initial node features `x` and the graph connectivity information `edge_index` to the model, and visualize its 2-dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, h = model(data_train.x, data_train.edge_index)\n",
    "print(f'Embedding shape: {list(h.shape)}')\n",
    "\n",
    "visualize_embedding(h, color=data_train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5f944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40841772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript  # Restrict height of output cell.\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data_train.x, data_train.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out, data_train.y)  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data_valid.x, data_valid.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred == data_test.y  # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum()) / int(data_test.sum())  # Derive ratio of correct predictions.\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32837850",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_valid_set = augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e3a0f",
   "metadata": {},
   "source": [
    "We now see that this process with basic data preparation, modelling and integration of the results in a DataFrame (as storage of the model) is very fast (in seconds):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebefe615",
   "metadata": {},
   "source": [
    "features_importances_from_pickle(\n",
    "    augmented_train_valid_set=augmented_train_valid_set,\n",
    "    X_train_valid=X_train_valid,\n",
    "    model_task=model_task,\n",
    "    uncorrected_model_path=uncorrected_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "print(f\"Basic modelling took {round(t1 - t0)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c6a49",
   "metadata": {},
   "source": [
    "The further steps are for fairness assessment and correction of the model, functionality which is available with the package FairDream of DreamQuark (private for the moment)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48be396",
   "metadata": {},
   "source": [
    "## Detection alert (on train&valid data to examine if the model learned discriminant behavior)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4108de8c",
   "metadata": {},
   "source": [
    "augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddfaf680",
   "metadata": {},
   "source": [
    "train_valid_set_with_uncorrected_results = augment_train_valid_set_with_results(\"uncorrected\", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1da81ac9",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "augmented_train_valid_set = train_valid_set_with_uncorrected_results\n",
    "model_name = \"uncorrected\"\n",
    "\n",
    "fairness_purpose='percentage_positive'\n",
    "injustice_acceptance=1\n",
    "min_individuals_discrimined=0.01\n",
    "\n",
    "discrimination_alert(augmented_train_valid_set, model_name, fairness_purpose, model_task, injustice_acceptance, min_individuals_discrimined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6fb59",
   "metadata": {},
   "source": [
    "## Discrimination correction with a new fair model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbadc17",
   "metadata": {},
   "source": [
    "### Generating fairer models with grid search or weights distorsion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5e1dad5",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# the user determines one's fairness objectives to build new fairer models\n",
    "# on which group and regarding which criteria (purpose, constraint of the models) one aims to erase discrimination\n",
    "\n",
    "protected_attribute = 'education-num'\n",
    "\n",
    "# then the user sets the desired balance between stat and fair performances \n",
    "tradeoff = \"moderate\"\n",
    "weight_method = 'grid_and_weighted_groups'\n",
    "nb_fair_models = 6\n",
    "\n",
    "\n",
    "train_valid_set_with_corrected_results, models_df, best_model_dict = fair_train(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    train_valid_set_with_uncorrected_results=train_valid_set_with_uncorrected_results,\n",
    "    protected_attribute=protected_attribute,\n",
    "    fairness_purpose=fairness_purpose,\n",
    "    model_task=model_task,\n",
    "    stat_criteria=stat_criteria,\n",
    "    tradeoff=tradeoff,\n",
    "    weight_method=weight_method,\n",
    "    nb_fair_models=nb_fair_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35c752",
   "metadata": {},
   "source": [
    "### Evaluating the best fair model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39cd5d2d",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fair_model_results(train_valid_set_with_corrected_results, models_df, best_model_dict,protected_attribute,fairness_purpose, model_task)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3cfd4f",
   "metadata": {},
   "source": [
    "top_models = models_df.sort_values(by='tradeoff_score',ascending=False)\n",
    "top_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

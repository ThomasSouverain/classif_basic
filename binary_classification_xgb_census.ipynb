# %%
%load_ext autoreload
%autoreload 2

import warnings
warnings.filterwarnings('ignore')

# %% [markdown]
# # Discrimination detection and mitigation (on revenues classification dataset)

# %% [markdown]
# ## Train a model regardless of fairness

# %%
import sys
sys.path.append("../")

from sklearn import datasets

from data_preparation import *
from model import *
from model_analysis import *

# %%
# set your statistics purposes
model_task = 'classification'
stat_criteria = 'auc'

# %% [markdown]
# ### Prepare data
# 
# Fix precise % of population distribution (sex: Male, Female) and % of loan granted according to sex, to inspect the effects of 

# %%
# preparing the dataset on clients for binary classification
from datasets import fetch_openml
data = fetch_openml(data_id=1590, as_frame=True)

X = data.data
Y = (data.target == '>50K') * 1

# %%
dataset = X.copy()
dataset['target'] = Y
dataset

# %%
# here, "treatment" is saw as being 'Male' and not 'Female'

df_response_if_feature=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==1)]
df_no_response_if_feature=dataset.loc[(dataset['sex']=='Male')&(dataset['target']==0)]
df_response_if_not_feature=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==1)]
df_no_response_if_not_feature=dataset.loc[(dataset['sex']=='Female')&(dataset['target']==0)]

print(df_response_if_feature.shape[0])
print(df_no_response_if_feature.shape[0])
print(df_response_if_not_feature.shape[0])
print(df_no_response_if_not_feature.shape[0])


# % of men selected by the initial data
df_response_if_feature.shape[0]/(df_response_if_feature.shape[0]+df_no_response_if_feature.shape[0])

# %%
# % of women selected by the initial data
df_response_if_not_feature.shape[0]/(df_response_if_not_feature.shape[0]+df_no_response_if_not_feature.shape[0])

# %%
len_dataset = 20_000

is_feature='Male',
percentage_feature= 50
is_response= 'target'
percentage_response_if_feature=70
percentage_response_if_not_feature=10

sexist_dataset = set_marketing_treatment_effect(df_response_if_feature=df_response_if_feature,
    df_no_response_if_feature=df_no_response_if_feature,
    df_response_if_not_feature=df_response_if_not_feature,
    df_no_response_if_not_feature=df_no_response_if_not_feature,
    len_dataset=len_dataset,
    is_feature=is_feature,
    percentage_feature=percentage_feature,
    is_response=is_response,
    percentage_response_if_feature=percentage_response_if_feature,
    percentage_response_if_not_feature=percentage_response_if_not_feature)

# %%
X = sexist_dataset.loc[: , dataset.columns != 'target']
Y = sexist_dataset['target']

# %%
Y

# %% [markdown]
# ### Bring your own model 

# %% [markdown]
# If you want to bring your own model, you have to set 3 features:
# 
# 1. uncorrected_model_path
# Save your model in uncorrected_model_path, for fairness analysis on relevant features
# Ex: uncorrected_model_path = "/work/data/models/uncorrected_model.pkl"
# 
# 2. X_train_valid, Y_train_valid
# pd.DataFrame with your inputs and targets on train&valid set, of shape(nb_individuals,)
# 
# 3. Y_pred_train_valid
# np.ndarray with the predicted label (i.e. class) or value, of shape(nb_individuals,)

# %% [markdown]
# ### Automatically train a model statistically performant, regardless of fairness

# %%
X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test = train_valid_test_split(X,Y, model_task)

# %%
Y_valid.shape

# %%
# save the uncorrected model, to then sort its features by importances
save_model=True
uncorrected_model_path = "/work/data/models/uncorrected_model.pkl"

Y_pred_train_valid = train_naive_xgb(X_train, X_valid, X_train_valid, X_test, Y_train, Y_valid, Y_train_valid, Y_test, model_task, stat_criteria, save_model=save_model)

# %% [markdown]
# ## Detection alert (on train&valid data to examine if the model learned discriminant behavior)

# %%
augment_train_valid_set_with_results("uncorrected", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)

# %%
train_valid_set_with_uncorrected_results = augment_train_valid_set_with_results("uncorrected", X_train_valid, Y_train_valid, Y_pred_train_valid, model_task)

# %%
augmented_train_valid_set = train_valid_set_with_uncorrected_results
model_name = "uncorrected"

fairness_purpose='percentage_positive'
injustice_acceptance=1
min_individuals_discrimined=0.01

discrimination_alert(augmented_train_valid_set, model_name, fairness_purpose, model_task, injustice_acceptance, min_individuals_discrimined)

# %% [markdown]
# ## Discrimination correction with a new fair model

# %% [markdown]
# ### Generating fairer models with grid search or weights distorsion

# %%
# the user determines one's fairness objectives to build new fairer models
# on which group and regarding which criteria (purpose, constraint of the models) one aims to erase discrimination

protected_attribute = 'education-num'

# then the user sets the desired balance between stat and fair performances 
tradeoff = "moderate"
weight_method = 'grid_and_weighted_groups'
nb_fair_models = 6


train_valid_set_with_corrected_results, models_df, best_model_dict = fair_train(
    X=X,
    Y=Y,
    train_valid_set_with_uncorrected_results=train_valid_set_with_uncorrected_results,
    protected_attribute=protected_attribute,
    fairness_purpose=fairness_purpose,
    model_task=model_task,
    stat_criteria=stat_criteria,
    tradeoff=tradeoff,
    weight_method=weight_method,
    nb_fair_models=nb_fair_models,
)

# %% [markdown]
# ### Evaluating the best fair model

# %%
fair_model_results(train_valid_set_with_corrected_results, models_df, best_model_dict,protected_attribute,fairness_purpose, model_task)

# %%
top_models = models_df.sort_values(by='tradeoff_score',ascending=False)
top_models
